{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoteBook for the first project\n",
    "- Je pensais que l'on pouvait essayer de faire ca comme ceci:\n",
    "    - Chacun de nous a un block (peut bien entendu en ajouter en dessous du sien) afin de ne pas avoir trop de conflit lorsque l'on git push\n",
    "        - A tester cette semaine pour voir si c'est ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stefan's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from proj1_helpers import * \n",
    "from play_with_data import * # Contains some functions that allow us to look the data and manipulate a little bit\n",
    "from pre_processing import * # Contains all the functions needed to do some pre-processing on the data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********** Import csv file ***********\n",
    "\n",
    "# Import of train.csv --> This was my way of doing it\n",
    "# id_train, labels_train, features_train = load_data(\"train.csv\")\n",
    "\n",
    "# By using the helpers \n",
    "data_path = \"train.csv\"\n",
    "labels_train, features_train, id_train = load_csv_data(data_path)\n",
    "\n",
    "# Import of test.csv --> This is my way of doing it, because it is not done in proj1_helpers\n",
    "id_test, features_test = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Data whitening and \"build model\" ***********\n",
    "# I think the order of the two functions below is important, because if we apply the whitening on the column of 1 it doesn't make sense I thin, do you???????\n",
    "\n",
    "features_train = data_whitening(features_train) # to whiten the data\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_train = build_model_data(features_train, labels_train)[1] # the \"[1]\" is to take only the \"X\" matrix and not the \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_bad_ind = np.unique(np.where(features_train == -999.0)[1])\n",
    "features_train = np.delete(features_train,features_bad_ind,1)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(features_train.shape[1]):\n",
    "    for j in list(range(i + 1)):    \n",
    "        newFeature = np.multiply(features_train[:,i],features_train[:,j])\n",
    "        features_train = np.c_[features_train,newFeature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a cross validation with logistic_regression on the data without the features with -999.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters:\n",
    "- Gamma\n",
    "- K --> number of sets for the cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cross_val import *\n",
    "\n",
    "# Prepare the data, remove the features with at least one value at -999.0\n",
    "data_features_removed = remove_features(features_train)\n",
    "\n",
    "seed = 1\n",
    "degree = 5 # No more useful\n",
    "k_fold = 15 # I have seen a decreasing in the error when increasing the number of fold\n",
    "gammas = np.linspace(-0.5,0.5,10)\n",
    "#lambdas = np.logspace(-4, 7, 40)# No longer useful because we don't use anymore the ridge regression\n",
    "\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(labels_train, k_fold, seed)\n",
    "# define lists to store the loss of training data and test data\n",
    "allValidationError = []\n",
    "\n",
    "for g in gammas: # To see with which gamma we obtained a better result\n",
    "    validation_error = 0\n",
    "    for k in range(k_fold):\n",
    "        validation_error = validation_error + cross_validation(labels_train, data_features_removed, k_indices, k, g) \n",
    "    allValidationError.append(validation_error/k_fold) # we compute the mean of the validation error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to have some information about the cross validation we just did\n",
    "plt.close(\"all\")\n",
    "plt.plot(gammas, allValidationError )\n",
    "print(allValidationError)\n",
    "print(min(allValidationError))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FUNCTIONS --> but not so sure of them! Maybe need to re-do (I can put the one I've did from the exercise, I'll do this Friday afternon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To compute the losse, with mse --> with solution\n",
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e)) # Here if we have any problem we should try with a transpose\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "\n",
    "    # compute gradient and loss\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -1/len(y) * np.dot(tx.T, e)\n",
    "    \n",
    "    loss = compute_mse(y,tx,w)\n",
    "    return gradient, loss\n",
    "\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient, loss = compute_gradient(y,tx,w)\n",
    "\n",
    "        # update w by gradient\n",
    "        w = ws[n_iter] - gamma*gradient\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses[-1], ws[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "            \n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -1/len(y) * np.dot(tx.T, e)\n",
    "    \n",
    "    loss = compute_loss(y,tx,w)\n",
    "    return gradient, loss\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    lossForOneRun = 0\n",
    "    \n",
    "    for n_iters in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient, loss = compute_stoch_gradient(minibatch_y, minibatch_tx, ws[n_iters])\n",
    "            lossForOneRun = lossForOneRun + loss       #Faudrait pas reinitialiser avant chaque iteration\n",
    "            w = ws[n_iters] - gamma*gradient\n",
    "        \n",
    "        losses.append((1/batch_size)*lossForOneRun)\n",
    "        ws.append(w)\n",
    "    \n",
    "    return losses[-1], ws[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares --> With solution\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    \n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ridge_regression using normal equations --> with solution\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "\n",
    "    aI = 2 * tx.shape[0] * lambda_*np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    w = np.linalg.solve(a,b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etienne's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"train.csv\")\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "display(trainData.head(5))\n",
    "display(testData.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = trainData.drop(['Id','Prediction'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Visualization of the data in order to have a first insight of the features and the distribution of non computable(\"NC\")\n",
    "values i.e -999.000\n",
    "'''\n",
    "NCFeatures = []\n",
    "NCValues = []\n",
    "for i in range(trainData.shape[1]):\n",
    "\n",
    "        b = np.where(features_train[:,i] <= -999.000)[0]\n",
    "        if (len(b) >= 1000):\n",
    "            \n",
    "            NCFeatures.append(i)\n",
    "            NCValues.append(len(b))\n",
    "            b = 0\n",
    "            \n",
    "        print('Feature ' + str(i) + ' Histogram')\n",
    "        plt.hist(trainData.iloc[:,i])\n",
    "        plt.show()         \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(NCValues, index = NCFeatures)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivan's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from proj1_helpers import * \n",
    "from play_with_data import * # Contains some functions that allow us to look the data and manipulate a little bit\n",
    "from pre_processing import * # Contains all the functions needed to do some pre-processing on the data\n",
    "from logistic_regression import * # Contains the functions for gradient descent, penalized gradient descent, stochastic gradient descent\n",
    "from cross_val import * # Contains the functions for the cross-validation\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********** Import csv file ***********\n",
    "\n",
    "# Import of train.csv --> This was my way of doing it\n",
    "# id_train, labels_train, features_train = load_data(\"train.csv\")\n",
    "\n",
    "# By using the helpers \n",
    "data_path = \"train.csv\"\n",
    "labels_train, features_train, id_train = load_csv_data(data_path)\n",
    "labels_train[np.where(labels_train == -1)[0]] = 0\n",
    "\n",
    "# Import of test.csv --> This is my way of doing it, because it is not done in proj1_helpers\n",
    "#id_test, features_test = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Data whitening and \"build model\" ***********\n",
    "# I think the order of the two functions below is important, because if we apply the whitening on the column of 1 it doesn't make sense I thin, do you???????\n",
    "\n",
    "features_train = data_whitening(features_train) # to whiten the data\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_train = build_model_data(features_train, labels_train)[1] # the \"[1]\" is to take only the \"X\" matrix and not the \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 20)\n"
     ]
    }
   ],
   "source": [
    "features_bad_ind = np.unique(np.where(features_train == -999.0)[1])\n",
    "features_train = np.delete(features_train,features_bad_ind,1)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Adding combination of features as new features at the power of choice. Ex (x1x2)^coef, (x1x3)^coef. \n",
    "x1^pow is not calculated here.\n",
    "'''\n",
    "def twoFeatureCombinationPower(data, coef, featNum):\n",
    "    \n",
    "    for i in range(featNum):\n",
    "        for j in list(range(i + 1)):\n",
    "            if i!=j:\n",
    "                newFeature = np.multiply(data[:,i],data[:,j])\n",
    "                for k in range(coef):\n",
    "                    newFeaturePow = np.power(newFeature,k+1)\n",
    "                    data = np.c_[data,newFeaturePow]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Adding combination of features as new features. Ex x1^2 is calculated here\n",
    "'''\n",
    "def FeaturePower(data, coef,featNum):\n",
    "    if coef <= 1:\n",
    "        print('No need to use this function you will duplicate features')\n",
    "    else:    \n",
    "        powers = np.linspace(2,coef,coef-1)    \n",
    "        for i in range(featNum):        \n",
    "            newFeature = data[:,i]\n",
    "            for j in powers:\n",
    "                newFeaturePow = np.power(newFeature,j)\n",
    "                data = np.c_[data,newFeaturePow]\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 400)\n",
      "(250000, 460)\n"
     ]
    }
   ],
   "source": [
    "tempData = features_train\n",
    "data = twoFeatureCombinationPower(tempData,2,features_train.shape[1])\n",
    "print(data.shape)\n",
    "data = FeaturePower(data,4,features_train.shape[1])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=labels_train\n",
    "tx=data\n",
    "initial_w=np.zeros((tx.shape[1], 1))\n",
    "max_iter=100\n",
    "gamma=1\n",
    "lambda_=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.8\tlambda: 5000000.0\n",
      "0\n",
      "Gradient Descent(0/4): loss=[[ 10397.2077084]]\n",
      "Gradient Descent(1/4): loss=[[ 10220.82394707]]\n",
      "Gradient Descent(2/4): loss=[[ 10219.23514497]]\n",
      "Gradient Descent(3/4): loss=[[ 10203.82495764]]\n",
      "Gradient Descent(4/4): loss=[[ 10241.91603657]]\n",
      "(25000, 1) (25000, 1)\n",
      "0.65936\n",
      "1\n",
      "Gradient Descent(0/4): loss=[[ 10397.2077084]]\n",
      "Gradient Descent(1/4): loss=[[ 10214.60335459]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Etienne/Documents/EPFL/Master/Master 3/ML/Project 1 ML/ML_project1/Notebook/logistic_regression.py:22: RuntimeWarning: overflow encountered in exp\n",
      "  loss_ = np.sum(np.log(1+np.exp(tx.dot(w))))-y.T.dot(tx.dot(w))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-503f79c0c2a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mallValidationError\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_l\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master/Master 3/ML/Project 1 ML/ML_project1/Notebook/cross_val.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, gamma, lambda_)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;31m#ÃŸround(y_train.shape[0]/batch_size) # Put it higher BUT add some condition for convergence (threshold)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalized_stochastic_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m#z = x_train.dot(w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master/Master 3/ML/Project 1 ML/ML_project1/Notebook/logistic_regression.py\u001b[0m in \u001b[0;36mpenalized_stochastic_gradient_descent\u001b[0;34m(y, tx, initial_w, batch_size, max_iter, gamma, lambda_)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatchy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchtx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# store w and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master/Master 3/ML/Project 1 ML/ML_project1/Notebook/logistic_regression.py\u001b[0m in \u001b[0;36mpenalized_logistic_regression\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m\"\"\"return the loss, gradient, and hessian.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Add the elements due to penalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master/Master 3/ML/Project 1 ML/ML_project1/Notebook/logistic_regression.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mhessian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalculate_hessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master/Master 3/ML/Project 1 ML/ML_project1/Notebook/logistic_regression.py\u001b[0m in \u001b[0;36mcalculate_hessian\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mdia\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "k_fold = 10\n",
    "gammas = 0.8 #np.linspace(0.25,1,4) # Hyperparameter\n",
    "lambdas = np.logspace(1,8,6)*5 # Hyperparameter\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "allValidationError = np.zeros([1, len(lambdas)])\n",
    "\n",
    "#for ind_g, g in enumerate(gammas):\n",
    "for ind_l, l in enumerate(lambdas):\n",
    "    validation_error = 0\n",
    "    print(\"gamma: {}\\tlambda: {}\".format(gammas,l))\n",
    "    for k in range(k_fold):\n",
    "        print(k)\n",
    "        validation_error = validation_error + cross_validation(y, tx, k_indices, k, gammas, l)\n",
    "    allValidationError[0, ind_l] = validation_error/k_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "a_pred = np.array([1,1,1,1,1,0,0])\n",
    "b_test = np.array([1,1,0,1,0,0,0])\n",
    "validation_error = np.where(a_pred != b_test)[0].shape[0]/len(b_test)\n",
    "print(validation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(allValidationError/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w,loss=learning_by_penalized_gradient(y, tx, initial_w, max_iter, gamma, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=7000\n",
    "w,loss=penalized_stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iter, gamma, lambda_)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rendering submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_test, features_test = load_data(\"test.csv\")\n",
    "\n",
    "features_train = data_whitening(features_test) # to whiten the data\n",
    "features_train = nonComputableValuesIntegration(features_test)\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_test = build_model_data(features_test) # the \"[1]\" is to take only the \"X\" matrix and not the \"y\"\n",
    "\n",
    "tempData = features_test\n",
    "data = twoFeatureCombinationPower(tempData,1,features_test.shape[1])\n",
    "data = FeaturePower(data,2,features_test.shape[1])\n",
    "\n",
    "\n",
    "y_pred = predict_labels(w1, data[:,:])\n",
    "create_csv_submission(id_test, y_pred, 'first_submission')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
