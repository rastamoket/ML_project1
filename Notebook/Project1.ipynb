{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoteBook for the first project\n",
    "- Je pensais que l'on pouvait essayer de faire ca comme ceci:\n",
    "    - Chacun de nous a un block (peut bien entendu en ajouter en dessous du sien) afin de ne pas avoir trop de conflit lorsque l'on git push\n",
    "        - A tester cette semaine pour voir si c'est ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stefan's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from proj1_helpers import * \n",
    "from play_with_data import * # Contains some functions that allow us to look the data and manipulate a little bit\n",
    "from pre_processing import * # Contains all the functions needed to do some pre-processing on the data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********** Import csv file ***********\n",
    "\n",
    "# Import of train.csv --> This was my way of doing it\n",
    "# id_train, labels_train, features_train = load_data(\"train.csv\")\n",
    "\n",
    "# By using the helpers \n",
    "data_path = \"train.csv\"\n",
    "labels_train, features_train, id_train = load_csv_data(data_path)\n",
    "\n",
    "# Import of test.csv --> This is my way of doing it, because it is not done in proj1_helpers\n",
    "id_test, features_test = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Data whitening and \"build model\" ***********\n",
    "# I think the order of the two functions below is important, because if we apply the whitening on the column of 1 it doesn't make sense I thin, do you???????\n",
    "\n",
    "features_train = data_whitening(features_train) # to whiten the data\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_train = build_model_data(features_train, labels_train)[1] # the \"[1]\" is to take only the \"X\" matrix and not the \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_bad_ind = np.unique(np.where(features_train == -999.0)[1])\n",
    "features_train = np.delete(features_train,features_bad_ind,1)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(features_train.shape[1]):\n",
    "    for j in list(range(i + 1)):    \n",
    "        newFeature = np.multiply(features_train[:,i],features_train[:,j])\n",
    "        features_train = np.c_[features_train,newFeature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a cross validation with logistic_regression on the data without the features with -999.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters:\n",
    "- Gamma\n",
    "- K --> number of sets for the cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cross_val import *\n",
    "\n",
    "# Prepare the data, remove the features with at least one value at -999.0\n",
    "data_features_removed = remove_features(features_train)\n",
    "\n",
    "seed = 1\n",
    "degree = 5 # No more useful\n",
    "k_fold = 15 # I have seen a decreasing in the error when increasing the number of fold\n",
    "gammas = np.linspace(-0.5,0.5,10)\n",
    "#lambdas = np.logspace(-4, 7, 40)# No longer useful because we don't use anymore the ridge regression\n",
    "\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(labels_train, k_fold, seed)\n",
    "# define lists to store the loss of training data and test data\n",
    "allValidationError = []\n",
    "\n",
    "for g in gammas: # To see with which gamma we obtained a better result\n",
    "    validation_error = 0\n",
    "    for k in range(k_fold):\n",
    "        validation_error = validation_error + cross_validation(labels_train, data_features_removed, k_indices, k, g) \n",
    "    allValidationError.append(validation_error/k_fold) # we compute the mean of the validation error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to have some information about the cross validation we just did\n",
    "plt.close(\"all\")\n",
    "plt.plot(gammas, allValidationError )\n",
    "print(allValidationError)\n",
    "print(min(allValidationError))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FUNCTIONS --> but not so sure of them! Maybe need to re-do (I can put the one I've did from the exercise, I'll do this Friday afternon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To compute the losse, with mse --> with solution\n",
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e)) # Here if we have any problem we should try with a transpose\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "\n",
    "    # compute gradient and loss\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -1/len(y) * np.dot(tx.T, e)\n",
    "    \n",
    "    loss = compute_mse(y,tx,w)\n",
    "    return gradient, loss\n",
    "\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient, loss = compute_gradient(y,tx,w)\n",
    "\n",
    "        # update w by gradient\n",
    "        w = ws[n_iter] - gamma*gradient\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses[-1], ws[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "            \n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -1/len(y) * np.dot(tx.T, e)\n",
    "    \n",
    "    loss = compute_loss(y,tx,w)\n",
    "    return gradient, loss\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    lossForOneRun = 0\n",
    "    \n",
    "    for n_iters in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient, loss = compute_stoch_gradient(minibatch_y, minibatch_tx, ws[n_iters])\n",
    "            lossForOneRun = lossForOneRun + loss       #Faudrait pas reinitialiser avant chaque iteration\n",
    "            w = ws[n_iters] - gamma*gradient\n",
    "        \n",
    "        losses.append((1/batch_size)*lossForOneRun)\n",
    "        ws.append(w)\n",
    "    \n",
    "    return losses[-1], ws[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares --> With solution\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    \n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ridge_regression using normal equations --> with solution\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "\n",
    "    aI = 2 * tx.shape[0] * lambda_*np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    w = np.linalg.solve(a,b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etienne's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"train.csv\")\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "display(trainData.head(5))\n",
    "display(testData.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = trainData.drop(['Id','Prediction'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Visualization of the data in order to have a first insight of the features and the distribution of non computable(\"NC\")\n",
    "values i.e -999.000\n",
    "'''\n",
    "NCFeatures = []\n",
    "NCValues = []\n",
    "for i in range(trainData.shape[1]):\n",
    "\n",
    "        b = np.where(features_train[:,i] <= -999.000)[0]\n",
    "        if (len(b) >= 1000):\n",
    "            \n",
    "            NCFeatures.append(i)\n",
    "            NCValues.append(len(b))\n",
    "            b = 0\n",
    "            \n",
    "        print('Feature ' + str(i) + ' Histogram')\n",
    "        plt.hist(trainData.iloc[:,i])\n",
    "        plt.show()         \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(NCValues, index = NCFeatures)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivan's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from proj1_helpers import * \n",
    "from play_with_data import * # Contains some functions that allow us to look the data and manipulate a little bit\n",
    "from pre_processing import * # Contains all the functions needed to do some pre-processing on the data\n",
    "from logistic_regression import * # Contains the functions for gradient descent, penalized gradient descent, stochastic gradient descent\n",
    "from cross_val import * # Contains the functions for the cross-validation\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********** Import csv file ***********\n",
    "\n",
    "# Import of train.csv --> This was my way of doing it\n",
    "# id_train, labels_train, features_train = load_data(\"train.csv\")\n",
    "\n",
    "# By using the helpers \n",
    "data_path = \"train.csv\"\n",
    "labels_train, features_train, id_train = load_csv_data(data_path)\n",
    "labels_train[np.where(labels_train == -1)[0]] = 0\n",
    "\n",
    "# Import of test.csv --> This is my way of doing it, because it is not done in proj1_helpers\n",
    "#id_test, features_test = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Data whitening and \"build model\" ***********\n",
    "# I think the order of the two functions below is important, because if we apply the whitening on the column of 1 it doesn't make sense I thin, do you???????\n",
    "\n",
    "features_train = data_whitening(features_train) # to whiten the data\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_train = build_model_data(features_train, labels_train)[1] # the \"[1]\" is to take only the \"X\" matrix and not the \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 20)\n"
     ]
    }
   ],
   "source": [
    "features_bad_ind = np.unique(np.where(features_train == -999.0)[1])\n",
    "features_train = np.delete(features_train,features_bad_ind,1)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Adding combination of features as new features at the power of choice. Ex (x1x2)^coef, (x1x3)^coef. \n",
    "x1^pow is not calculated here.\n",
    "'''\n",
    "def twoFeatureCombinationPower(data, coef, featNum):\n",
    "    \n",
    "    for i in range(featNum):\n",
    "        for j in list(range(i + 1)):\n",
    "            if i!=j:\n",
    "                newFeature = np.multiply(data[:,i],data[:,j])\n",
    "                for k in range(coef):\n",
    "                    newFeaturePow = np.power(newFeature,k+1)\n",
    "                    data = np.c_[data,newFeaturePow]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Adding combination of features as new features. Ex x1^2 is calculated here\n",
    "'''\n",
    "def FeaturePower(data, coef,featNum):\n",
    "    if coef <= 1:\n",
    "        print('No need to use this function you will duplicate features')\n",
    "    else:    \n",
    "        powers = np.linspace(2,coef,coef-1)    \n",
    "        for i in range(featNum):        \n",
    "            newFeature = data[:,i]\n",
    "            for j in powers:\n",
    "                newFeaturePow = np.power(newFeature,j)\n",
    "                data = np.c_[data,newFeaturePow]\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 210)\n",
      "(250000, 230)\n"
     ]
    }
   ],
   "source": [
    "tempData = features_train\n",
    "data = twoFeatureCombinationPower(tempData,1,features_train.shape[1])\n",
    "print(data.shape)\n",
    "data = FeaturePower(data,2,features_train.shape[1])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=labels_train\n",
    "tx=data\n",
    "initial_w=np.zeros((tx.shape[1], 1))\n",
    "max_iter=100\n",
    "gamma=1\n",
    "lambda_=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.25\tlambda: 0.1\n",
      "0\n",
      "Gradient Descent(0/999): loss=[[ 4852.03026392]]\n",
      "Gradient Descent(1/999): loss=[[ 4255.13369617]]\n",
      "Gradient Descent(2/999): loss=[[ 3899.01307459]]\n",
      "Gradient Descent(3/999): loss=[[ 3650.16533576]]\n",
      "Gradient Descent(4/999): loss=[[ 3908.92022693]]\n",
      "Gradient Descent(5/999): loss=[[ 7888.56194775]]\n",
      "Gradient Descent(6/999): loss=[[ 4211.46384433]]\n",
      "Gradient Descent(7/999): loss=[[ 3444.85261525]]\n",
      "Gradient Descent(8/999): loss=[[ 3289.67588832]]\n",
      "Gradient Descent(9/999): loss=[[ 3262.18200682]]\n",
      "Gradient Descent(10/999): loss=[[ 3138.80043315]]\n",
      "Gradient Descent(11/999): loss=[[ 3213.50070733]]\n",
      "Gradient Descent(12/999): loss=[[ 3613.27376342]]\n",
      "Gradient Descent(13/999): loss=[[ 9513.4092797]]\n",
      "Gradient Descent(14/999): loss=[[ 8537.93795444]]\n",
      "Gradient Descent(15/999): loss=[[ 5936.4451265]]\n",
      "Gradient Descent(16/999): loss=[[ 4462.58124813]]\n",
      "Gradient Descent(17/999): loss=[[ 3884.67268952]]\n",
      "Gradient Descent(18/999): loss=[[ 3556.96141914]]\n",
      "Gradient Descent(19/999): loss=[[ 3438.74906049]]\n",
      "Gradient Descent(20/999): loss=[[ 3276.37808662]]\n",
      "Gradient Descent(21/999): loss=[[ 3144.04887248]]\n",
      "Gradient Descent(22/999): loss=[[ 3161.59858178]]\n",
      "Gradient Descent(23/999): loss=[[ 3050.79154385]]\n",
      "Gradient Descent(24/999): loss=[[ 3082.82042515]]\n",
      "Gradient Descent(25/999): loss=[[ 3317.41980817]]\n",
      "Gradient Descent(26/999): loss=[[ 3132.55678387]]\n",
      "Gradient Descent(27/999): loss=[[ 3163.08804614]]\n",
      "Gradient Descent(28/999): loss=[[ 3113.64797673]]\n",
      "Gradient Descent(29/999): loss=[[ 3035.44242278]]\n",
      "Gradient Descent(30/999): loss=[[ 3031.03413901]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stefan\\OneDrive\\EPFL\\OneNote\\Master\\Ma-3\\Machine Learning\\Project1\\ML_project1\\Notebook\\logistic_regression.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (np.exp(-x) + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/999): loss=[[ 3859.94352929]]\n",
      "Gradient Descent(32/999): loss=[[ 10471.45488884]]\n",
      "Gradient Descent(33/999): loss=[[ 12935.88179431]]\n",
      "Gradient Descent(34/999): loss=[[ 17490.68449313]]\n",
      "Gradient Descent(35/999): loss=[[ 41201.08354248]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stefan\\OneDrive\\EPFL\\OneNote\\Master\\Ma-3\\Machine Learning\\Project1\\ML_project1\\Notebook\\logistic_regression.py:22: RuntimeWarning: overflow encountered in exp\n",
      "  loss_ = np.sum(np.log(1+np.exp(tx.dot(w))))-y.T.dot(tx.dot(w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/999): loss=[[ inf]]\n",
      "Gradient Descent(37/999): loss=[[ inf]]\n",
      "Gradient Descent(38/999): loss=[[ inf]]\n",
      "Gradient Descent(39/999): loss=[[ inf]]\n",
      "Gradient Descent(40/999): loss=[[ inf]]\n",
      "Gradient Descent(41/999): loss=[[ inf]]\n",
      "Gradient Descent(42/999): loss=[[  7.63367601e+08]]\n",
      "Gradient Descent(43/999): loss=[[ inf]]\n",
      "Gradient Descent(44/999): loss=[[  2.95771093e+08]]\n",
      "Gradient Descent(45/999): loss=[[ inf]]\n",
      "Gradient Descent(46/999): loss=[[  7.88319095e+08]]\n",
      "Gradient Descent(47/999): loss=[[ inf]]\n",
      "Gradient Descent(48/999): loss=[[  3.35834437e+08]]\n",
      "Gradient Descent(49/999): loss=[[ inf]]\n",
      "Gradient Descent(50/999): loss=[[  7.84309710e+08]]\n",
      "Gradient Descent(51/999): loss=[[ inf]]\n",
      "Gradient Descent(52/999): loss=[[ inf]]\n",
      "Gradient Descent(53/999): loss=[[ inf]]\n",
      "Gradient Descent(54/999): loss=[[  6.69805189e+08]]\n",
      "Gradient Descent(55/999): loss=[[ inf]]\n",
      "Gradient Descent(56/999): loss=[[ inf]]\n",
      "Gradient Descent(57/999): loss=[[ inf]]\n",
      "Gradient Descent(58/999): loss=[[  7.59330384e+08]]\n",
      "Gradient Descent(59/999): loss=[[ inf]]\n",
      "Gradient Descent(60/999): loss=[[ inf]]\n",
      "Gradient Descent(61/999): loss=[[ inf]]\n",
      "Gradient Descent(62/999): loss=[[ inf]]\n",
      "Gradient Descent(63/999): loss=[[ inf]]\n",
      "Gradient Descent(64/999): loss=[[ inf]]\n",
      "Gradient Descent(65/999): loss=[[ inf]]\n",
      "Gradient Descent(66/999): loss=[[  7.29161726e+08]]\n",
      "Gradient Descent(67/999): loss=[[ inf]]\n",
      "Gradient Descent(68/999): loss=[[ inf]]\n",
      "Gradient Descent(69/999): loss=[[ inf]]\n",
      "Gradient Descent(70/999): loss=[[  7.49963449e+08]]\n",
      "Gradient Descent(71/999): loss=[[ inf]]\n",
      "Gradient Descent(72/999): loss=[[ inf]]\n",
      "Gradient Descent(73/999): loss=[[ inf]]\n",
      "Gradient Descent(74/999): loss=[[  8.18999460e+08]]\n",
      "Gradient Descent(75/999): loss=[[ inf]]\n",
      "Gradient Descent(76/999): loss=[[ inf]]\n",
      "Gradient Descent(77/999): loss=[[ inf]]\n",
      "Gradient Descent(78/999): loss=[[  7.55075144e+08]]\n",
      "Gradient Descent(79/999): loss=[[ inf]]\n",
      "Gradient Descent(80/999): loss=[[ inf]]\n",
      "Gradient Descent(81/999): loss=[[ inf]]\n",
      "Gradient Descent(82/999): loss=[[  6.99114371e+08]]\n",
      "Gradient Descent(83/999): loss=[[ inf]]\n",
      "Gradient Descent(84/999): loss=[[ inf]]\n",
      "Gradient Descent(85/999): loss=[[ inf]]\n",
      "Gradient Descent(86/999): loss=[[  6.84782762e+08]]\n",
      "Gradient Descent(87/999): loss=[[ inf]]\n",
      "Gradient Descent(88/999): loss=[[ inf]]\n",
      "Gradient Descent(89/999): loss=[[ inf]]\n",
      "Gradient Descent(90/999): loss=[[  7.65775560e+08]]\n",
      "Gradient Descent(91/999): loss=[[ inf]]\n",
      "Gradient Descent(92/999): loss=[[ inf]]\n",
      "Gradient Descent(93/999): loss=[[ inf]]\n",
      "Gradient Descent(94/999): loss=[[  7.02286731e+08]]\n",
      "Gradient Descent(95/999): loss=[[ inf]]\n",
      "Gradient Descent(96/999): loss=[[  3.43705207e+08]]\n",
      "Gradient Descent(97/999): loss=[[ inf]]\n",
      "Gradient Descent(98/999): loss=[[  6.22820804e+08]]\n",
      "Gradient Descent(99/999): loss=[[ inf]]\n",
      "Gradient Descent(100/999): loss=[[ inf]]\n",
      "Gradient Descent(101/999): loss=[[ inf]]\n",
      "Gradient Descent(102/999): loss=[[  8.58873096e+08]]\n",
      "Gradient Descent(103/999): loss=[[ inf]]\n",
      "Gradient Descent(104/999): loss=[[ inf]]\n",
      "Gradient Descent(105/999): loss=[[ inf]]\n",
      "Gradient Descent(106/999): loss=[[  9.82883292e+08]]\n",
      "Gradient Descent(107/999): loss=[[ inf]]\n",
      "Gradient Descent(108/999): loss=[[ inf]]\n",
      "Gradient Descent(109/999): loss=[[ inf]]\n",
      "Gradient Descent(110/999): loss=[[ inf]]\n",
      "Gradient Descent(111/999): loss=[[ inf]]\n",
      "Gradient Descent(112/999): loss=[[  4.45964416e+08]]\n",
      "Gradient Descent(113/999): loss=[[ inf]]\n",
      "Gradient Descent(114/999): loss=[[  8.07409720e+08]]\n",
      "Gradient Descent(115/999): loss=[[ inf]]\n",
      "Gradient Descent(116/999): loss=[[ inf]]\n",
      "Gradient Descent(117/999): loss=[[ inf]]\n",
      "Gradient Descent(118/999): loss=[[ inf]]\n",
      "Gradient Descent(119/999): loss=[[ inf]]\n",
      "Gradient Descent(120/999): loss=[[  3.80944102e+08]]\n",
      "Gradient Descent(121/999): loss=[[ inf]]\n",
      "Gradient Descent(122/999): loss=[[  8.55444838e+08]]\n",
      "Gradient Descent(123/999): loss=[[ inf]]\n",
      "Gradient Descent(124/999): loss=[[ inf]]\n",
      "Gradient Descent(125/999): loss=[[ inf]]\n",
      "Gradient Descent(126/999): loss=[[  7.51219527e+08]]\n",
      "Gradient Descent(127/999): loss=[[ inf]]\n",
      "Gradient Descent(128/999): loss=[[ inf]]\n",
      "Gradient Descent(129/999): loss=[[ inf]]\n",
      "Gradient Descent(130/999): loss=[[ inf]]\n",
      "Gradient Descent(131/999): loss=[[ inf]]\n",
      "Gradient Descent(132/999): loss=[[ inf]]\n",
      "Gradient Descent(133/999): loss=[[ inf]]\n",
      "Gradient Descent(134/999): loss=[[  8.21217054e+08]]\n",
      "Gradient Descent(135/999): loss=[[ inf]]\n",
      "Gradient Descent(136/999): loss=[[ inf]]\n",
      "Gradient Descent(137/999): loss=[[ inf]]\n",
      "Gradient Descent(138/999): loss=[[  7.59134436e+08]]\n",
      "Gradient Descent(139/999): loss=[[ inf]]\n",
      "Gradient Descent(140/999): loss=[[ inf]]\n",
      "Gradient Descent(141/999): loss=[[ inf]]\n",
      "Gradient Descent(142/999): loss=[[  7.60536342e+08]]\n",
      "Gradient Descent(143/999): loss=[[ inf]]\n",
      "Gradient Descent(144/999): loss=[[ inf]]\n",
      "Gradient Descent(145/999): loss=[[ inf]]\n",
      "Gradient Descent(146/999): loss=[[  7.51006207e+08]]\n",
      "Gradient Descent(147/999): loss=[[ inf]]\n",
      "Gradient Descent(148/999): loss=[[ inf]]\n",
      "Gradient Descent(149/999): loss=[[ inf]]\n",
      "Gradient Descent(150/999): loss=[[  7.63595158e+08]]\n",
      "Gradient Descent(151/999): loss=[[ inf]]\n",
      "Gradient Descent(152/999): loss=[[ inf]]\n",
      "Gradient Descent(153/999): loss=[[ inf]]\n",
      "Gradient Descent(154/999): loss=[[  6.85141657e+08]]\n",
      "Gradient Descent(155/999): loss=[[ inf]]\n",
      "Gradient Descent(156/999): loss=[[ inf]]\n",
      "Gradient Descent(157/999): loss=[[ inf]]\n",
      "Gradient Descent(158/999): loss=[[  7.34940287e+08]]\n",
      "Gradient Descent(159/999): loss=[[ inf]]\n",
      "Gradient Descent(160/999): loss=[[ inf]]\n",
      "Gradient Descent(161/999): loss=[[ inf]]\n",
      "Gradient Descent(162/999): loss=[[  7.34611903e+08]]\n",
      "Gradient Descent(163/999): loss=[[ inf]]\n",
      "Gradient Descent(164/999): loss=[[ inf]]\n",
      "Gradient Descent(165/999): loss=[[ inf]]\n",
      "Gradient Descent(166/999): loss=[[  8.94030772e+08]]\n",
      "Gradient Descent(167/999): loss=[[ inf]]\n",
      "Gradient Descent(168/999): loss=[[ inf]]\n",
      "Gradient Descent(169/999): loss=[[ inf]]\n",
      "Gradient Descent(170/999): loss=[[ inf]]\n",
      "Gradient Descent(171/999): loss=[[ inf]]\n",
      "Gradient Descent(172/999): loss=[[ inf]]\n",
      "Gradient Descent(173/999): loss=[[ inf]]\n",
      "Gradient Descent(174/999): loss=[[  7.89410402e+08]]\n",
      "Gradient Descent(175/999): loss=[[ inf]]\n",
      "Gradient Descent(176/999): loss=[[ inf]]\n",
      "Gradient Descent(177/999): loss=[[ inf]]\n",
      "Gradient Descent(178/999): loss=[[  7.63601108e+08]]\n",
      "Gradient Descent(179/999): loss=[[ inf]]\n",
      "Gradient Descent(180/999): loss=[[ inf]]\n",
      "Gradient Descent(181/999): loss=[[ inf]]\n",
      "Gradient Descent(182/999): loss=[[  7.82780779e+08]]\n",
      "Gradient Descent(183/999): loss=[[ inf]]\n",
      "Gradient Descent(184/999): loss=[[ inf]]\n",
      "Gradient Descent(185/999): loss=[[ inf]]\n",
      "Gradient Descent(186/999): loss=[[  7.27611128e+08]]\n",
      "Gradient Descent(187/999): loss=[[ inf]]\n",
      "Gradient Descent(188/999): loss=[[ inf]]\n",
      "Gradient Descent(189/999): loss=[[ inf]]\n",
      "Gradient Descent(190/999): loss=[[  7.07771307e+08]]\n",
      "Gradient Descent(191/999): loss=[[ inf]]\n",
      "Gradient Descent(192/999): loss=[[ inf]]\n",
      "Gradient Descent(193/999): loss=[[ inf]]\n",
      "Gradient Descent(194/999): loss=[[  7.01685158e+08]]\n",
      "Gradient Descent(195/999): loss=[[ inf]]\n",
      "Gradient Descent(196/999): loss=[[ inf]]\n",
      "Gradient Descent(197/999): loss=[[ inf]]\n",
      "Gradient Descent(198/999): loss=[[  7.64177278e+08]]\n",
      "Gradient Descent(199/999): loss=[[ inf]]\n",
      "Gradient Descent(200/999): loss=[[ inf]]\n",
      "Gradient Descent(201/999): loss=[[ inf]]\n",
      "Gradient Descent(202/999): loss=[[  6.96758010e+08]]\n",
      "Gradient Descent(203/999): loss=[[ inf]]\n",
      "Gradient Descent(204/999): loss=[[ inf]]\n",
      "Gradient Descent(205/999): loss=[[ inf]]\n",
      "Gradient Descent(206/999): loss=[[  9.77560792e+08]]\n",
      "Gradient Descent(207/999): loss=[[ inf]]\n",
      "Gradient Descent(208/999): loss=[[ inf]]\n",
      "Gradient Descent(209/999): loss=[[ inf]]\n",
      "Gradient Descent(210/999): loss=[[ inf]]\n",
      "Gradient Descent(211/999): loss=[[ inf]]\n",
      "Gradient Descent(212/999): loss=[[  4.06536124e+08]]\n",
      "Gradient Descent(213/999): loss=[[ inf]]\n",
      "Gradient Descent(214/999): loss=[[  6.71231767e+08]]\n",
      "Gradient Descent(215/999): loss=[[ inf]]\n",
      "Gradient Descent(216/999): loss=[[ inf]]\n",
      "Gradient Descent(217/999): loss=[[ inf]]\n",
      "Gradient Descent(218/999): loss=[[  8.63439285e+08]]\n",
      "Gradient Descent(219/999): loss=[[ inf]]\n",
      "Gradient Descent(220/999): loss=[[ inf]]\n",
      "Gradient Descent(221/999): loss=[[ inf]]\n",
      "Gradient Descent(222/999): loss=[[  6.67933314e+08]]\n",
      "Gradient Descent(223/999): loss=[[ inf]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(224/999): loss=[[ inf]]\n",
      "Gradient Descent(225/999): loss=[[ inf]]\n",
      "Gradient Descent(226/999): loss=[[  6.48792386e+08]]\n",
      "Gradient Descent(227/999): loss=[[ inf]]\n",
      "Gradient Descent(228/999): loss=[[ inf]]\n",
      "Gradient Descent(229/999): loss=[[ inf]]\n",
      "Gradient Descent(230/999): loss=[[  7.61618294e+08]]\n",
      "Gradient Descent(231/999): loss=[[ inf]]\n",
      "Gradient Descent(232/999): loss=[[ inf]]\n",
      "Gradient Descent(233/999): loss=[[ inf]]\n",
      "Gradient Descent(234/999): loss=[[  8.20148214e+08]]\n",
      "Gradient Descent(235/999): loss=[[ inf]]\n",
      "Gradient Descent(236/999): loss=[[ inf]]\n",
      "Gradient Descent(237/999): loss=[[ inf]]\n",
      "Gradient Descent(238/999): loss=[[  7.97238431e+08]]\n",
      "Gradient Descent(239/999): loss=[[ inf]]\n",
      "Gradient Descent(240/999): loss=[[ inf]]\n",
      "Gradient Descent(241/999): loss=[[ inf]]\n",
      "Gradient Descent(242/999): loss=[[  6.41356846e+08]]\n",
      "Gradient Descent(243/999): loss=[[ inf]]\n",
      "Gradient Descent(244/999): loss=[[ inf]]\n",
      "Gradient Descent(245/999): loss=[[ inf]]\n",
      "Gradient Descent(246/999): loss=[[ inf]]\n",
      "Gradient Descent(247/999): loss=[[ inf]]\n",
      "Gradient Descent(248/999): loss=[[ inf]]\n",
      "Gradient Descent(249/999): loss=[[ inf]]\n",
      "Gradient Descent(250/999): loss=[[  7.67444840e+08]]\n",
      "Gradient Descent(251/999): loss=[[ inf]]\n",
      "Gradient Descent(252/999): loss=[[  4.17029049e+08]]\n",
      "Gradient Descent(253/999): loss=[[ inf]]\n",
      "Gradient Descent(254/999): loss=[[  7.77990397e+08]]\n",
      "Gradient Descent(255/999): loss=[[ inf]]\n",
      "Gradient Descent(256/999): loss=[[ inf]]\n",
      "Gradient Descent(257/999): loss=[[ inf]]\n",
      "Gradient Descent(258/999): loss=[[ inf]]\n",
      "Gradient Descent(259/999): loss=[[ inf]]\n",
      "Gradient Descent(260/999): loss=[[  7.37233424e+08]]\n",
      "Gradient Descent(261/999): loss=[[ inf]]\n",
      "Gradient Descent(262/999): loss=[[ inf]]\n",
      "Gradient Descent(263/999): loss=[[ inf]]\n",
      "Gradient Descent(264/999): loss=[[  6.63697499e+08]]\n",
      "Gradient Descent(265/999): loss=[[ inf]]\n",
      "Gradient Descent(266/999): loss=[[ inf]]\n",
      "Gradient Descent(267/999): loss=[[ inf]]\n",
      "Gradient Descent(268/999): loss=[[  7.43724567e+08]]\n",
      "Gradient Descent(269/999): loss=[[ inf]]\n",
      "Gradient Descent(270/999): loss=[[ inf]]\n",
      "Gradient Descent(271/999): loss=[[ inf]]\n",
      "Gradient Descent(272/999): loss=[[  7.16714909e+08]]\n",
      "Gradient Descent(273/999): loss=[[ inf]]\n",
      "Gradient Descent(274/999): loss=[[  3.65898537e+08]]\n",
      "Gradient Descent(275/999): loss=[[ inf]]\n",
      "Gradient Descent(276/999): loss=[[  8.38422084e+08]]\n",
      "Gradient Descent(277/999): loss=[[ inf]]\n",
      "Gradient Descent(278/999): loss=[[ inf]]\n",
      "Gradient Descent(279/999): loss=[[ inf]]\n",
      "Gradient Descent(280/999): loss=[[  7.20733635e+08]]\n",
      "Gradient Descent(281/999): loss=[[ inf]]\n",
      "Gradient Descent(282/999): loss=[[ inf]]\n",
      "Gradient Descent(283/999): loss=[[ inf]]\n",
      "Gradient Descent(284/999): loss=[[  7.54002371e+08]]\n",
      "Gradient Descent(285/999): loss=[[ inf]]\n",
      "Gradient Descent(286/999): loss=[[ inf]]\n",
      "Gradient Descent(287/999): loss=[[ inf]]\n",
      "Gradient Descent(288/999): loss=[[  7.87598419e+08]]\n",
      "Gradient Descent(289/999): loss=[[ inf]]\n",
      "Gradient Descent(290/999): loss=[[ inf]]\n",
      "Gradient Descent(291/999): loss=[[ inf]]\n",
      "Gradient Descent(292/999): loss=[[  7.29882714e+08]]\n",
      "Gradient Descent(293/999): loss=[[ inf]]\n",
      "Gradient Descent(294/999): loss=[[ inf]]\n",
      "Gradient Descent(295/999): loss=[[ inf]]\n",
      "Gradient Descent(296/999): loss=[[  6.86538590e+08]]\n",
      "Gradient Descent(297/999): loss=[[ inf]]\n",
      "Gradient Descent(298/999): loss=[[ inf]]\n",
      "Gradient Descent(299/999): loss=[[ inf]]\n",
      "Gradient Descent(300/999): loss=[[  6.15209698e+08]]\n",
      "Gradient Descent(301/999): loss=[[ inf]]\n",
      "Gradient Descent(302/999): loss=[[ inf]]\n",
      "Gradient Descent(303/999): loss=[[ inf]]\n",
      "Gradient Descent(304/999): loss=[[  8.03088953e+08]]\n",
      "Gradient Descent(305/999): loss=[[ inf]]\n",
      "Gradient Descent(306/999): loss=[[ inf]]\n",
      "Gradient Descent(307/999): loss=[[ inf]]\n",
      "Gradient Descent(308/999): loss=[[  6.42924449e+08]]\n",
      "Gradient Descent(309/999): loss=[[ inf]]\n",
      "Gradient Descent(310/999): loss=[[ inf]]\n",
      "Gradient Descent(311/999): loss=[[ inf]]\n",
      "Gradient Descent(312/999): loss=[[  8.15481311e+08]]\n",
      "Gradient Descent(313/999): loss=[[ inf]]\n",
      "Gradient Descent(314/999): loss=[[ inf]]\n",
      "Gradient Descent(315/999): loss=[[ inf]]\n",
      "Gradient Descent(316/999): loss=[[  6.82405135e+08]]\n",
      "Gradient Descent(317/999): loss=[[ inf]]\n",
      "Gradient Descent(318/999): loss=[[ inf]]\n",
      "Gradient Descent(319/999): loss=[[ inf]]\n",
      "Gradient Descent(320/999): loss=[[  7.58865473e+08]]\n",
      "Gradient Descent(321/999): loss=[[ inf]]\n",
      "Gradient Descent(322/999): loss=[[ inf]]\n",
      "Gradient Descent(323/999): loss=[[ inf]]\n",
      "Gradient Descent(324/999): loss=[[  8.43676066e+08]]\n",
      "Gradient Descent(325/999): loss=[[ inf]]\n",
      "Gradient Descent(326/999): loss=[[ inf]]\n",
      "Gradient Descent(327/999): loss=[[ inf]]\n",
      "Gradient Descent(328/999): loss=[[  7.00353403e+08]]\n",
      "Gradient Descent(329/999): loss=[[ inf]]\n",
      "Gradient Descent(330/999): loss=[[ inf]]\n",
      "Gradient Descent(331/999): loss=[[ inf]]\n",
      "Gradient Descent(332/999): loss=[[  7.71995170e+08]]\n",
      "Gradient Descent(333/999): loss=[[ inf]]\n",
      "Gradient Descent(334/999): loss=[[  3.95514189e+08]]\n",
      "Gradient Descent(335/999): loss=[[ inf]]\n",
      "Gradient Descent(336/999): loss=[[  7.94001361e+08]]\n",
      "Gradient Descent(337/999): loss=[[ inf]]\n",
      "Gradient Descent(338/999): loss=[[ inf]]\n",
      "Gradient Descent(339/999): loss=[[ inf]]\n",
      "Gradient Descent(340/999): loss=[[  7.14578185e+08]]\n",
      "Gradient Descent(341/999): loss=[[ inf]]\n",
      "Gradient Descent(342/999): loss=[[ inf]]\n",
      "Gradient Descent(343/999): loss=[[ inf]]\n",
      "Gradient Descent(344/999): loss=[[  6.96039872e+08]]\n",
      "Gradient Descent(345/999): loss=[[ inf]]\n",
      "Gradient Descent(346/999): loss=[[ inf]]\n",
      "Gradient Descent(347/999): loss=[[ inf]]\n",
      "Gradient Descent(348/999): loss=[[  7.15493392e+08]]\n",
      "Gradient Descent(349/999): loss=[[ inf]]\n",
      "Gradient Descent(350/999): loss=[[ inf]]\n",
      "Gradient Descent(351/999): loss=[[ inf]]\n",
      "Gradient Descent(352/999): loss=[[ inf]]\n",
      "Gradient Descent(353/999): loss=[[ inf]]\n",
      "Gradient Descent(354/999): loss=[[ inf]]\n",
      "Gradient Descent(355/999): loss=[[ inf]]\n",
      "Gradient Descent(356/999): loss=[[  7.64421329e+08]]\n",
      "Gradient Descent(357/999): loss=[[ inf]]\n",
      "Gradient Descent(358/999): loss=[[ inf]]\n",
      "Gradient Descent(359/999): loss=[[ inf]]\n",
      "Gradient Descent(360/999): loss=[[  8.03221133e+08]]\n",
      "Gradient Descent(361/999): loss=[[ inf]]\n",
      "Gradient Descent(362/999): loss=[[ inf]]\n",
      "Gradient Descent(363/999): loss=[[ inf]]\n",
      "Gradient Descent(364/999): loss=[[  7.42685880e+08]]\n",
      "Gradient Descent(365/999): loss=[[ inf]]\n",
      "Gradient Descent(366/999): loss=[[ inf]]\n",
      "Gradient Descent(367/999): loss=[[ inf]]\n",
      "Gradient Descent(368/999): loss=[[  9.73513406e+08]]\n",
      "Gradient Descent(369/999): loss=[[ inf]]\n",
      "Gradient Descent(370/999): loss=[[ inf]]\n",
      "Gradient Descent(371/999): loss=[[ inf]]\n",
      "Gradient Descent(372/999): loss=[[ inf]]\n",
      "Gradient Descent(373/999): loss=[[ inf]]\n",
      "Gradient Descent(374/999): loss=[[ inf]]\n",
      "Gradient Descent(375/999): loss=[[ inf]]\n",
      "Gradient Descent(376/999): loss=[[  6.12723683e+08]]\n",
      "Gradient Descent(377/999): loss=[[ inf]]\n",
      "Gradient Descent(378/999): loss=[[  6.03289184e+08]]\n",
      "Gradient Descent(379/999): loss=[[ inf]]\n",
      "Gradient Descent(380/999): loss=[[  7.90790903e+08]]\n",
      "Gradient Descent(381/999): loss=[[ inf]]\n",
      "Gradient Descent(382/999): loss=[[  3.78773702e+08]]\n",
      "Gradient Descent(383/999): loss=[[ inf]]\n",
      "Gradient Descent(384/999): loss=[[  8.12944475e+08]]\n",
      "Gradient Descent(385/999): loss=[[ inf]]\n",
      "Gradient Descent(386/999): loss=[[ inf]]\n",
      "Gradient Descent(387/999): loss=[[ inf]]\n",
      "Gradient Descent(388/999): loss=[[  6.62895763e+08]]\n",
      "Gradient Descent(389/999): loss=[[ inf]]\n",
      "Gradient Descent(390/999): loss=[[ inf]]\n",
      "Gradient Descent(391/999): loss=[[ inf]]\n",
      "Gradient Descent(392/999): loss=[[  9.05399416e+08]]\n",
      "Gradient Descent(393/999): loss=[[ inf]]\n",
      "Gradient Descent(394/999): loss=[[ inf]]\n",
      "Gradient Descent(395/999): loss=[[ inf]]\n",
      "Gradient Descent(396/999): loss=[[ inf]]\n",
      "Gradient Descent(397/999): loss=[[ inf]]\n",
      "Gradient Descent(398/999): loss=[[ inf]]\n",
      "Gradient Descent(399/999): loss=[[ inf]]\n",
      "Gradient Descent(400/999): loss=[[  5.83319596e+08]]\n",
      "Gradient Descent(401/999): loss=[[ inf]]\n",
      "Gradient Descent(402/999): loss=[[  4.13441841e+08]]\n",
      "Gradient Descent(403/999): loss=[[ inf]]\n",
      "Gradient Descent(404/999): loss=[[  7.98559298e+08]]\n",
      "Gradient Descent(405/999): loss=[[ inf]]\n",
      "Gradient Descent(406/999): loss=[[ inf]]\n",
      "Gradient Descent(407/999): loss=[[ inf]]\n",
      "Gradient Descent(408/999): loss=[[  7.20875776e+08]]\n",
      "Gradient Descent(409/999): loss=[[ inf]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(410/999): loss=[[ inf]]\n",
      "Gradient Descent(411/999): loss=[[ inf]]\n",
      "Gradient Descent(412/999): loss=[[  7.25819710e+08]]\n",
      "Gradient Descent(413/999): loss=[[ inf]]\n",
      "Gradient Descent(414/999): loss=[[  3.84702104e+08]]\n",
      "Gradient Descent(415/999): loss=[[ inf]]\n",
      "Gradient Descent(416/999): loss=[[  7.01289147e+08]]\n",
      "Gradient Descent(417/999): loss=[[ inf]]\n",
      "Gradient Descent(418/999): loss=[[ inf]]\n",
      "Gradient Descent(419/999): loss=[[ inf]]\n",
      "Gradient Descent(420/999): loss=[[  7.28888102e+08]]\n",
      "Gradient Descent(421/999): loss=[[ inf]]\n",
      "Gradient Descent(422/999): loss=[[ inf]]\n",
      "Gradient Descent(423/999): loss=[[ inf]]\n",
      "Gradient Descent(424/999): loss=[[  7.50249104e+08]]\n",
      "Gradient Descent(425/999): loss=[[ inf]]\n",
      "Gradient Descent(426/999): loss=[[ inf]]\n",
      "Gradient Descent(427/999): loss=[[ inf]]\n",
      "Gradient Descent(428/999): loss=[[  7.67035004e+08]]\n",
      "Gradient Descent(429/999): loss=[[ inf]]\n",
      "Gradient Descent(430/999): loss=[[ inf]]\n",
      "Gradient Descent(431/999): loss=[[ inf]]\n",
      "Gradient Descent(432/999): loss=[[  7.00766318e+08]]\n",
      "Gradient Descent(433/999): loss=[[ inf]]\n",
      "Gradient Descent(434/999): loss=[[ inf]]\n",
      "Gradient Descent(435/999): loss=[[ inf]]\n",
      "Gradient Descent(436/999): loss=[[  7.52869085e+08]]\n",
      "Gradient Descent(437/999): loss=[[ inf]]\n",
      "Gradient Descent(438/999): loss=[[ inf]]\n",
      "Gradient Descent(439/999): loss=[[ inf]]\n",
      "Gradient Descent(440/999): loss=[[  7.38971810e+08]]\n",
      "Gradient Descent(441/999): loss=[[ inf]]\n",
      "Gradient Descent(442/999): loss=[[  4.29456706e+08]]\n",
      "Gradient Descent(443/999): loss=[[ inf]]\n",
      "Gradient Descent(444/999): loss=[[  7.92128189e+08]]\n",
      "Gradient Descent(445/999): loss=[[ inf]]\n",
      "Gradient Descent(446/999): loss=[[ inf]]\n",
      "Gradient Descent(447/999): loss=[[ inf]]\n",
      "Gradient Descent(448/999): loss=[[  7.20097395e+08]]\n",
      "Gradient Descent(449/999): loss=[[ inf]]\n",
      "Gradient Descent(450/999): loss=[[ inf]]\n",
      "Gradient Descent(451/999): loss=[[ inf]]\n",
      "Gradient Descent(452/999): loss=[[  7.44211292e+08]]\n",
      "Gradient Descent(453/999): loss=[[ inf]]\n",
      "Gradient Descent(454/999): loss=[[ inf]]\n",
      "Gradient Descent(455/999): loss=[[ inf]]\n",
      "Gradient Descent(456/999): loss=[[  7.59989256e+08]]\n",
      "Gradient Descent(457/999): loss=[[ inf]]\n",
      "Gradient Descent(458/999): loss=[[ inf]]\n",
      "Gradient Descent(459/999): loss=[[ inf]]\n",
      "Gradient Descent(460/999): loss=[[  7.37467781e+08]]\n",
      "Gradient Descent(461/999): loss=[[ inf]]\n",
      "Gradient Descent(462/999): loss=[[ inf]]\n",
      "Gradient Descent(463/999): loss=[[ inf]]\n",
      "Gradient Descent(464/999): loss=[[  9.68639391e+08]]\n",
      "Gradient Descent(465/999): loss=[[ inf]]\n",
      "Gradient Descent(466/999): loss=[[ inf]]\n",
      "Gradient Descent(467/999): loss=[[ inf]]\n",
      "Gradient Descent(468/999): loss=[[ inf]]\n",
      "Gradient Descent(469/999): loss=[[ inf]]\n",
      "Gradient Descent(470/999): loss=[[  6.55979726e+08]]\n",
      "Gradient Descent(471/999): loss=[[ inf]]\n",
      "Gradient Descent(472/999): loss=[[ inf]]\n",
      "Gradient Descent(473/999): loss=[[ inf]]\n",
      "Gradient Descent(474/999): loss=[[  7.57849079e+08]]\n",
      "Gradient Descent(475/999): loss=[[ inf]]\n",
      "Gradient Descent(476/999): loss=[[ inf]]\n",
      "Gradient Descent(477/999): loss=[[ inf]]\n",
      "Gradient Descent(478/999): loss=[[  7.15887301e+08]]\n",
      "Gradient Descent(479/999): loss=[[ inf]]\n",
      "Gradient Descent(480/999): loss=[[ inf]]\n",
      "Gradient Descent(481/999): loss=[[ inf]]\n",
      "Gradient Descent(482/999): loss=[[  7.64165906e+08]]\n",
      "Gradient Descent(483/999): loss=[[ inf]]\n",
      "Gradient Descent(484/999): loss=[[ inf]]\n",
      "Gradient Descent(485/999): loss=[[ inf]]\n",
      "Gradient Descent(486/999): loss=[[ inf]]\n",
      "Gradient Descent(487/999): loss=[[ inf]]\n",
      "Gradient Descent(488/999): loss=[[ inf]]\n",
      "Gradient Descent(489/999): loss=[[ inf]]\n",
      "Gradient Descent(490/999): loss=[[  7.90019274e+08]]\n",
      "Gradient Descent(491/999): loss=[[ inf]]\n",
      "Gradient Descent(492/999): loss=[[ inf]]\n",
      "Gradient Descent(493/999): loss=[[ inf]]\n",
      "Gradient Descent(494/999): loss=[[  7.25931195e+08]]\n",
      "Gradient Descent(495/999): loss=[[ inf]]\n",
      "Gradient Descent(496/999): loss=[[  3.31318953e+08]]\n",
      "Gradient Descent(497/999): loss=[[ inf]]\n",
      "Gradient Descent(498/999): loss=[[  8.49284767e+08]]\n",
      "Gradient Descent(499/999): loss=[[ inf]]\n",
      "Gradient Descent(500/999): loss=[[ inf]]\n",
      "Gradient Descent(501/999): loss=[[ inf]]\n",
      "Gradient Descent(502/999): loss=[[ inf]]\n",
      "Gradient Descent(503/999): loss=[[ inf]]\n",
      "Gradient Descent(504/999): loss=[[  7.81320278e+08]]\n",
      "Gradient Descent(505/999): loss=[[ inf]]\n",
      "Gradient Descent(506/999): loss=[[ inf]]\n",
      "Gradient Descent(507/999): loss=[[ inf]]\n",
      "Gradient Descent(508/999): loss=[[ inf]]\n",
      "Gradient Descent(509/999): loss=[[ inf]]\n",
      "Gradient Descent(510/999): loss=[[ inf]]\n",
      "Gradient Descent(511/999): loss=[[ inf]]\n",
      "Gradient Descent(512/999): loss=[[  7.81139975e+08]]\n",
      "Gradient Descent(513/999): loss=[[ inf]]\n",
      "Gradient Descent(514/999): loss=[[ inf]]\n",
      "Gradient Descent(515/999): loss=[[ inf]]\n",
      "Gradient Descent(516/999): loss=[[  7.46337209e+08]]\n",
      "Gradient Descent(517/999): loss=[[ inf]]\n",
      "Gradient Descent(518/999): loss=[[ inf]]\n",
      "Gradient Descent(519/999): loss=[[ inf]]\n",
      "Gradient Descent(520/999): loss=[[  6.94809061e+08]]\n",
      "Gradient Descent(521/999): loss=[[ inf]]\n",
      "Gradient Descent(522/999): loss=[[ inf]]\n",
      "Gradient Descent(523/999): loss=[[ inf]]\n",
      "Gradient Descent(524/999): loss=[[ inf]]\n",
      "Gradient Descent(525/999): loss=[[ inf]]\n",
      "Gradient Descent(526/999): loss=[[ inf]]\n",
      "Gradient Descent(527/999): loss=[[ inf]]\n",
      "Gradient Descent(528/999): loss=[[  6.80343207e+08]]\n",
      "Gradient Descent(529/999): loss=[[ inf]]\n",
      "Gradient Descent(530/999): loss=[[ inf]]\n",
      "Gradient Descent(531/999): loss=[[ inf]]\n",
      "Gradient Descent(532/999): loss=[[  8.50315664e+08]]\n",
      "Gradient Descent(533/999): loss=[[ inf]]\n",
      "Gradient Descent(534/999): loss=[[ inf]]\n",
      "Gradient Descent(535/999): loss=[[ inf]]\n",
      "Gradient Descent(536/999): loss=[[  5.87231021e+08]]\n",
      "Gradient Descent(537/999): loss=[[ inf]]\n",
      "Gradient Descent(538/999): loss=[[  6.55628424e+08]]\n",
      "Gradient Descent(539/999): loss=[[ inf]]\n",
      "Gradient Descent(540/999): loss=[[  4.33077253e+08]]\n",
      "Gradient Descent(541/999): loss=[[ inf]]\n",
      "Gradient Descent(542/999): loss=[[  7.42053967e+08]]\n",
      "Gradient Descent(543/999): loss=[[ inf]]\n",
      "Gradient Descent(544/999): loss=[[ inf]]\n",
      "Gradient Descent(545/999): loss=[[ inf]]\n",
      "Gradient Descent(546/999): loss=[[  6.90621931e+08]]\n",
      "Gradient Descent(547/999): loss=[[ inf]]\n",
      "Gradient Descent(548/999): loss=[[ inf]]\n",
      "Gradient Descent(549/999): loss=[[ inf]]\n",
      "Gradient Descent(550/999): loss=[[  7.60002889e+08]]\n",
      "Gradient Descent(551/999): loss=[[ inf]]\n",
      "Gradient Descent(552/999): loss=[[ inf]]\n",
      "Gradient Descent(553/999): loss=[[ inf]]\n",
      "Gradient Descent(554/999): loss=[[  7.45184177e+08]]\n",
      "Gradient Descent(555/999): loss=[[ inf]]\n",
      "Gradient Descent(556/999): loss=[[ inf]]\n",
      "Gradient Descent(557/999): loss=[[ inf]]\n",
      "Gradient Descent(558/999): loss=[[  7.15466322e+08]]\n",
      "Gradient Descent(559/999): loss=[[ inf]]\n",
      "Gradient Descent(560/999): loss=[[ inf]]\n",
      "Gradient Descent(561/999): loss=[[ inf]]\n",
      "Gradient Descent(562/999): loss=[[  7.54066941e+08]]\n",
      "Gradient Descent(563/999): loss=[[ inf]]\n",
      "Gradient Descent(564/999): loss=[[ inf]]\n",
      "Gradient Descent(565/999): loss=[[ inf]]\n",
      "Gradient Descent(566/999): loss=[[  7.66951507e+08]]\n",
      "Gradient Descent(567/999): loss=[[ inf]]\n",
      "Gradient Descent(568/999): loss=[[ inf]]\n",
      "Gradient Descent(569/999): loss=[[ inf]]\n",
      "Gradient Descent(570/999): loss=[[  7.21101823e+08]]\n",
      "Gradient Descent(571/999): loss=[[ inf]]\n",
      "Gradient Descent(572/999): loss=[[ inf]]\n",
      "Gradient Descent(573/999): loss=[[ inf]]\n",
      "Gradient Descent(574/999): loss=[[  6.89082693e+08]]\n",
      "Gradient Descent(575/999): loss=[[ inf]]\n",
      "Gradient Descent(576/999): loss=[[ inf]]\n",
      "Gradient Descent(577/999): loss=[[ inf]]\n",
      "Gradient Descent(578/999): loss=[[  7.84872134e+08]]\n",
      "Gradient Descent(579/999): loss=[[ inf]]\n",
      "Gradient Descent(580/999): loss=[[ inf]]\n",
      "Gradient Descent(581/999): loss=[[ inf]]\n",
      "Gradient Descent(582/999): loss=[[  7.58772265e+08]]\n",
      "Gradient Descent(583/999): loss=[[ inf]]\n",
      "Gradient Descent(584/999): loss=[[ inf]]\n",
      "Gradient Descent(585/999): loss=[[ inf]]\n",
      "Gradient Descent(586/999): loss=[[  9.97289430e+08]]\n",
      "Gradient Descent(587/999): loss=[[ inf]]\n",
      "Gradient Descent(588/999): loss=[[ inf]]\n",
      "Gradient Descent(589/999): loss=[[ inf]]\n",
      "Gradient Descent(590/999): loss=[[ inf]]\n",
      "Gradient Descent(591/999): loss=[[ inf]]\n",
      "Gradient Descent(592/999): loss=[[  7.82148174e+08]]\n",
      "Gradient Descent(593/999): loss=[[ inf]]\n",
      "Gradient Descent(594/999): loss=[[ inf]]\n",
      "Gradient Descent(595/999): loss=[[ inf]]\n",
      "Gradient Descent(596/999): loss=[[  8.07115703e+08]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(597/999): loss=[[ inf]]\n",
      "Gradient Descent(598/999): loss=[[ inf]]\n",
      "Gradient Descent(599/999): loss=[[ inf]]\n",
      "Gradient Descent(600/999): loss=[[  7.21044353e+08]]\n",
      "Gradient Descent(601/999): loss=[[ inf]]\n",
      "Gradient Descent(602/999): loss=[[ inf]]\n",
      "Gradient Descent(603/999): loss=[[ inf]]\n",
      "Gradient Descent(604/999): loss=[[  6.25921614e+08]]\n",
      "Gradient Descent(605/999): loss=[[ inf]]\n",
      "Gradient Descent(606/999): loss=[[ inf]]\n",
      "Gradient Descent(607/999): loss=[[ inf]]\n",
      "Gradient Descent(608/999): loss=[[  7.70177922e+08]]\n",
      "Gradient Descent(609/999): loss=[[ inf]]\n",
      "Gradient Descent(610/999): loss=[[ inf]]\n",
      "Gradient Descent(611/999): loss=[[ inf]]\n",
      "Gradient Descent(612/999): loss=[[  7.22893546e+08]]\n",
      "Gradient Descent(613/999): loss=[[ inf]]\n",
      "Gradient Descent(614/999): loss=[[ inf]]\n",
      "Gradient Descent(615/999): loss=[[ inf]]\n",
      "Gradient Descent(616/999): loss=[[  7.07455111e+08]]\n",
      "Gradient Descent(617/999): loss=[[ inf]]\n",
      "Gradient Descent(618/999): loss=[[ inf]]\n",
      "Gradient Descent(619/999): loss=[[ inf]]\n",
      "Gradient Descent(620/999): loss=[[  6.03317953e+08]]\n",
      "Gradient Descent(621/999): loss=[[ inf]]\n",
      "Gradient Descent(622/999): loss=[[ inf]]\n",
      "Gradient Descent(623/999): loss=[[ inf]]\n",
      "Gradient Descent(624/999): loss=[[  7.12099474e+08]]\n",
      "Gradient Descent(625/999): loss=[[ inf]]\n",
      "Gradient Descent(626/999): loss=[[  3.49895671e+08]]\n",
      "Gradient Descent(627/999): loss=[[ inf]]\n",
      "Gradient Descent(628/999): loss=[[  7.46239247e+08]]\n",
      "Gradient Descent(629/999): loss=[[ inf]]\n",
      "Gradient Descent(630/999): loss=[[ inf]]\n",
      "Gradient Descent(631/999): loss=[[ inf]]\n",
      "Gradient Descent(632/999): loss=[[  7.39948309e+08]]\n",
      "Gradient Descent(633/999): loss=[[ inf]]\n",
      "Gradient Descent(634/999): loss=[[ inf]]\n",
      "Gradient Descent(635/999): loss=[[ inf]]\n",
      "Gradient Descent(636/999): loss=[[  6.63597926e+08]]\n",
      "Gradient Descent(637/999): loss=[[ inf]]\n",
      "Gradient Descent(638/999): loss=[[ inf]]\n",
      "Gradient Descent(639/999): loss=[[ inf]]\n",
      "Gradient Descent(640/999): loss=[[  6.73299096e+08]]\n",
      "Gradient Descent(641/999): loss=[[ inf]]\n",
      "Gradient Descent(642/999): loss=[[ inf]]\n",
      "Gradient Descent(643/999): loss=[[ inf]]\n",
      "Gradient Descent(644/999): loss=[[  7.11267718e+08]]\n",
      "Gradient Descent(645/999): loss=[[ inf]]\n",
      "Gradient Descent(646/999): loss=[[ inf]]\n",
      "Gradient Descent(647/999): loss=[[ inf]]\n",
      "Gradient Descent(648/999): loss=[[  6.68521773e+08]]\n",
      "Gradient Descent(649/999): loss=[[ inf]]\n",
      "Gradient Descent(650/999): loss=[[ inf]]\n",
      "Gradient Descent(651/999): loss=[[ inf]]\n",
      "Gradient Descent(652/999): loss=[[  6.83086310e+08]]\n",
      "Gradient Descent(653/999): loss=[[ inf]]\n",
      "Gradient Descent(654/999): loss=[[ inf]]\n",
      "Gradient Descent(655/999): loss=[[ inf]]\n",
      "Gradient Descent(656/999): loss=[[  7.79166895e+08]]\n",
      "Gradient Descent(657/999): loss=[[ inf]]\n",
      "Gradient Descent(658/999): loss=[[ inf]]\n",
      "Gradient Descent(659/999): loss=[[ inf]]\n",
      "Gradient Descent(660/999): loss=[[  6.63140044e+08]]\n",
      "Gradient Descent(661/999): loss=[[ inf]]\n",
      "Gradient Descent(662/999): loss=[[ inf]]\n",
      "Gradient Descent(663/999): loss=[[ inf]]\n",
      "Gradient Descent(664/999): loss=[[  7.67393814e+08]]\n",
      "Gradient Descent(665/999): loss=[[ inf]]\n",
      "Gradient Descent(666/999): loss=[[  3.40897051e+08]]\n",
      "Gradient Descent(667/999): loss=[[ inf]]\n",
      "Gradient Descent(668/999): loss=[[  7.19821636e+08]]\n",
      "Gradient Descent(669/999): loss=[[ inf]]\n",
      "Gradient Descent(670/999): loss=[[ inf]]\n",
      "Gradient Descent(671/999): loss=[[ inf]]\n",
      "Gradient Descent(672/999): loss=[[ inf]]\n",
      "Gradient Descent(673/999): loss=[[ inf]]\n",
      "Gradient Descent(674/999): loss=[[ inf]]\n",
      "Gradient Descent(675/999): loss=[[ inf]]\n",
      "Gradient Descent(676/999): loss=[[  7.01358098e+08]]\n",
      "Gradient Descent(677/999): loss=[[ inf]]\n",
      "Gradient Descent(678/999): loss=[[ inf]]\n",
      "Gradient Descent(679/999): loss=[[ inf]]\n",
      "Gradient Descent(680/999): loss=[[  7.81763418e+08]]\n",
      "Gradient Descent(681/999): loss=[[ inf]]\n",
      "Gradient Descent(682/999): loss=[[ inf]]\n",
      "Gradient Descent(683/999): loss=[[ inf]]\n",
      "Gradient Descent(684/999): loss=[[  7.66261598e+08]]\n",
      "Gradient Descent(685/999): loss=[[ inf]]\n",
      "Gradient Descent(686/999): loss=[[ inf]]\n",
      "Gradient Descent(687/999): loss=[[ inf]]\n",
      "Gradient Descent(688/999): loss=[[  6.65575764e+08]]\n",
      "Gradient Descent(689/999): loss=[[ inf]]\n",
      "Gradient Descent(690/999): loss=[[ inf]]\n",
      "Gradient Descent(691/999): loss=[[ inf]]\n",
      "Gradient Descent(692/999): loss=[[  7.25946622e+08]]\n",
      "Gradient Descent(693/999): loss=[[ inf]]\n",
      "Gradient Descent(694/999): loss=[[  3.31025814e+08]]\n",
      "Gradient Descent(695/999): loss=[[ inf]]\n",
      "Gradient Descent(696/999): loss=[[ inf]]\n",
      "Gradient Descent(697/999): loss=[[ inf]]\n",
      "Gradient Descent(698/999): loss=[[  6.08828931e+08]]\n",
      "Gradient Descent(699/999): loss=[[ inf]]\n",
      "Gradient Descent(700/999): loss=[[ inf]]\n",
      "Gradient Descent(701/999): loss=[[ inf]]\n",
      "Gradient Descent(702/999): loss=[[  8.07418710e+08]]\n",
      "Gradient Descent(703/999): loss=[[ inf]]\n",
      "Gradient Descent(704/999): loss=[[ inf]]\n",
      "Gradient Descent(705/999): loss=[[ inf]]\n",
      "Gradient Descent(706/999): loss=[[  7.58611441e+08]]\n",
      "Gradient Descent(707/999): loss=[[ inf]]\n",
      "Gradient Descent(708/999): loss=[[ inf]]\n",
      "Gradient Descent(709/999): loss=[[ inf]]\n",
      "Gradient Descent(710/999): loss=[[  6.43537745e+08]]\n",
      "Gradient Descent(711/999): loss=[[ inf]]\n",
      "Gradient Descent(712/999): loss=[[  4.20031754e+08]]\n",
      "Gradient Descent(713/999): loss=[[ inf]]\n",
      "Gradient Descent(714/999): loss=[[  8.59176432e+08]]\n",
      "Gradient Descent(715/999): loss=[[ inf]]\n",
      "Gradient Descent(716/999): loss=[[ inf]]\n",
      "Gradient Descent(717/999): loss=[[ inf]]\n",
      "Gradient Descent(718/999): loss=[[  7.41409535e+08]]\n",
      "Gradient Descent(719/999): loss=[[ inf]]\n",
      "Gradient Descent(720/999): loss=[[ inf]]\n",
      "Gradient Descent(721/999): loss=[[ inf]]\n",
      "Gradient Descent(722/999): loss=[[  6.77142719e+08]]\n",
      "Gradient Descent(723/999): loss=[[ inf]]\n",
      "Gradient Descent(724/999): loss=[[ inf]]\n",
      "Gradient Descent(725/999): loss=[[ inf]]\n",
      "Gradient Descent(726/999): loss=[[  7.63507005e+08]]\n",
      "Gradient Descent(727/999): loss=[[ inf]]\n",
      "Gradient Descent(728/999): loss=[[ inf]]\n",
      "Gradient Descent(729/999): loss=[[ inf]]\n",
      "Gradient Descent(730/999): loss=[[  6.82287360e+08]]\n",
      "Gradient Descent(731/999): loss=[[ inf]]\n",
      "Gradient Descent(732/999): loss=[[ inf]]\n",
      "Gradient Descent(733/999): loss=[[ inf]]\n",
      "Gradient Descent(734/999): loss=[[  6.80052457e+08]]\n",
      "Gradient Descent(735/999): loss=[[ inf]]\n",
      "Gradient Descent(736/999): loss=[[ inf]]\n",
      "Gradient Descent(737/999): loss=[[ inf]]\n",
      "Gradient Descent(738/999): loss=[[ inf]]\n",
      "Gradient Descent(739/999): loss=[[ inf]]\n",
      "Gradient Descent(740/999): loss=[[ inf]]\n",
      "Gradient Descent(741/999): loss=[[ inf]]\n",
      "Gradient Descent(742/999): loss=[[  7.20988447e+08]]\n",
      "Gradient Descent(743/999): loss=[[ inf]]\n",
      "Gradient Descent(744/999): loss=[[ inf]]\n",
      "Gradient Descent(745/999): loss=[[ inf]]\n",
      "Gradient Descent(746/999): loss=[[  8.36665521e+08]]\n",
      "Gradient Descent(747/999): loss=[[ inf]]\n",
      "Gradient Descent(748/999): loss=[[ inf]]\n",
      "Gradient Descent(749/999): loss=[[ inf]]\n",
      "Gradient Descent(750/999): loss=[[ inf]]\n",
      "Gradient Descent(751/999): loss=[[ inf]]\n",
      "Gradient Descent(752/999): loss=[[  6.39147713e+08]]\n",
      "Gradient Descent(753/999): loss=[[ inf]]\n",
      "Gradient Descent(754/999): loss=[[ inf]]\n",
      "Gradient Descent(755/999): loss=[[ inf]]\n",
      "Gradient Descent(756/999): loss=[[ inf]]\n",
      "Gradient Descent(757/999): loss=[[ inf]]\n",
      "Gradient Descent(758/999): loss=[[ inf]]\n",
      "Gradient Descent(759/999): loss=[[ inf]]\n",
      "Gradient Descent(760/999): loss=[[  7.28258185e+08]]\n",
      "Gradient Descent(761/999): loss=[[ inf]]\n",
      "Gradient Descent(762/999): loss=[[ inf]]\n",
      "Gradient Descent(763/999): loss=[[ inf]]\n",
      "Gradient Descent(764/999): loss=[[  6.65906880e+08]]\n",
      "Gradient Descent(765/999): loss=[[ inf]]\n",
      "Gradient Descent(766/999): loss=[[  4.59614409e+08]]\n",
      "Gradient Descent(767/999): loss=[[ inf]]\n",
      "Gradient Descent(768/999): loss=[[  7.56545130e+08]]\n",
      "Gradient Descent(769/999): loss=[[ inf]]\n",
      "Gradient Descent(770/999): loss=[[ inf]]\n",
      "Gradient Descent(771/999): loss=[[ inf]]\n",
      "Gradient Descent(772/999): loss=[[  6.62541309e+08]]\n",
      "Gradient Descent(773/999): loss=[[ inf]]\n",
      "Gradient Descent(774/999): loss=[[  3.36988102e+08]]\n",
      "Gradient Descent(775/999): loss=[[ inf]]\n",
      "Gradient Descent(776/999): loss=[[  6.83704650e+08]]\n",
      "Gradient Descent(777/999): loss=[[ inf]]\n",
      "Gradient Descent(778/999): loss=[[ inf]]\n",
      "Gradient Descent(779/999): loss=[[ inf]]\n",
      "Gradient Descent(780/999): loss=[[  7.84258053e+08]]\n",
      "Gradient Descent(781/999): loss=[[ inf]]\n",
      "Gradient Descent(782/999): loss=[[ inf]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(783/999): loss=[[ inf]]\n",
      "Gradient Descent(784/999): loss=[[  6.97929435e+08]]\n",
      "Gradient Descent(785/999): loss=[[ inf]]\n",
      "Gradient Descent(786/999): loss=[[ inf]]\n",
      "Gradient Descent(787/999): loss=[[ inf]]\n",
      "Gradient Descent(788/999): loss=[[ inf]]\n",
      "Gradient Descent(789/999): loss=[[ inf]]\n",
      "Gradient Descent(790/999): loss=[[ inf]]\n",
      "Gradient Descent(791/999): loss=[[ inf]]\n",
      "Gradient Descent(792/999): loss=[[  7.03497611e+08]]\n",
      "Gradient Descent(793/999): loss=[[ inf]]\n",
      "Gradient Descent(794/999): loss=[[ inf]]\n",
      "Gradient Descent(795/999): loss=[[ inf]]\n",
      "Gradient Descent(796/999): loss=[[  6.84852548e+08]]\n",
      "Gradient Descent(797/999): loss=[[ inf]]\n",
      "Gradient Descent(798/999): loss=[[ inf]]\n",
      "Gradient Descent(799/999): loss=[[ inf]]\n",
      "Gradient Descent(800/999): loss=[[  7.69770495e+08]]\n",
      "Gradient Descent(801/999): loss=[[ inf]]\n",
      "Gradient Descent(802/999): loss=[[ inf]]\n",
      "Gradient Descent(803/999): loss=[[ inf]]\n",
      "Gradient Descent(804/999): loss=[[  7.21921353e+08]]\n",
      "Gradient Descent(805/999): loss=[[ inf]]\n",
      "Gradient Descent(806/999): loss=[[ inf]]\n",
      "Gradient Descent(807/999): loss=[[ inf]]\n",
      "Gradient Descent(808/999): loss=[[  7.96191329e+08]]\n",
      "Gradient Descent(809/999): loss=[[ inf]]\n",
      "Gradient Descent(810/999): loss=[[ inf]]\n",
      "Gradient Descent(811/999): loss=[[ inf]]\n",
      "Gradient Descent(812/999): loss=[[  7.08939874e+08]]\n",
      "Gradient Descent(813/999): loss=[[ inf]]\n",
      "Gradient Descent(814/999): loss=[[ inf]]\n",
      "Gradient Descent(815/999): loss=[[ inf]]\n",
      "Gradient Descent(816/999): loss=[[  6.96932359e+08]]\n",
      "Gradient Descent(817/999): loss=[[ inf]]\n",
      "Gradient Descent(818/999): loss=[[ inf]]\n",
      "Gradient Descent(819/999): loss=[[ inf]]\n",
      "Gradient Descent(820/999): loss=[[  7.70120306e+08]]\n",
      "Gradient Descent(821/999): loss=[[ inf]]\n",
      "Gradient Descent(822/999): loss=[[ inf]]\n",
      "Gradient Descent(823/999): loss=[[ inf]]\n",
      "Gradient Descent(824/999): loss=[[  7.10991744e+08]]\n",
      "Gradient Descent(825/999): loss=[[ inf]]\n",
      "Gradient Descent(826/999): loss=[[ inf]]\n",
      "Gradient Descent(827/999): loss=[[ inf]]\n",
      "Gradient Descent(828/999): loss=[[  6.49909446e+08]]\n",
      "Gradient Descent(829/999): loss=[[ inf]]\n",
      "Gradient Descent(830/999): loss=[[  5.06165849e+08]]\n",
      "Gradient Descent(831/999): loss=[[ inf]]\n",
      "Gradient Descent(832/999): loss=[[  6.04818752e+08]]\n",
      "Gradient Descent(833/999): loss=[[ inf]]\n",
      "Gradient Descent(834/999): loss=[[  5.30843637e+08]]\n",
      "Gradient Descent(835/999): loss=[[ inf]]\n",
      "Gradient Descent(836/999): loss=[[  5.57674498e+08]]\n",
      "Gradient Descent(837/999): loss=[[ inf]]\n",
      "Gradient Descent(838/999): loss=[[  5.66803603e+08]]\n",
      "Gradient Descent(839/999): loss=[[ inf]]\n",
      "Gradient Descent(840/999): loss=[[ inf]]\n",
      "Gradient Descent(841/999): loss=[[ inf]]\n",
      "Gradient Descent(842/999): loss=[[  6.35918346e+08]]\n",
      "Gradient Descent(843/999): loss=[[ inf]]\n",
      "Gradient Descent(844/999): loss=[[  5.03283821e+08]]\n",
      "Gradient Descent(845/999): loss=[[ inf]]\n",
      "Gradient Descent(846/999): loss=[[  6.99657473e+08]]\n",
      "Gradient Descent(847/999): loss=[[ inf]]\n",
      "Gradient Descent(848/999): loss=[[ inf]]\n",
      "Gradient Descent(849/999): loss=[[ inf]]\n",
      "Gradient Descent(850/999): loss=[[  6.78282904e+08]]\n",
      "Gradient Descent(851/999): loss=[[ inf]]\n",
      "Gradient Descent(852/999): loss=[[ inf]]\n",
      "Gradient Descent(853/999): loss=[[ inf]]\n",
      "Gradient Descent(854/999): loss=[[  8.25769387e+08]]\n",
      "Gradient Descent(855/999): loss=[[ inf]]\n",
      "Gradient Descent(856/999): loss=[[ inf]]\n",
      "Gradient Descent(857/999): loss=[[ inf]]\n",
      "Gradient Descent(858/999): loss=[[  7.61341458e+08]]\n",
      "Gradient Descent(859/999): loss=[[ inf]]\n",
      "Gradient Descent(860/999): loss=[[  4.11776233e+08]]\n",
      "Gradient Descent(861/999): loss=[[ inf]]\n",
      "Gradient Descent(862/999): loss=[[  8.20356588e+08]]\n",
      "Gradient Descent(863/999): loss=[[ inf]]\n",
      "Gradient Descent(864/999): loss=[[  2.79668898e+08]]\n",
      "Gradient Descent(865/999): loss=[[ inf]]\n",
      "Gradient Descent(866/999): loss=[[  7.93922005e+08]]\n",
      "Gradient Descent(867/999): loss=[[ inf]]\n",
      "Gradient Descent(868/999): loss=[[ inf]]\n",
      "Gradient Descent(869/999): loss=[[ inf]]\n",
      "Gradient Descent(870/999): loss=[[  8.43124430e+08]]\n",
      "Gradient Descent(871/999): loss=[[ inf]]\n",
      "Gradient Descent(872/999): loss=[[  2.76213935e+08]]\n",
      "Gradient Descent(873/999): loss=[[ inf]]\n",
      "Gradient Descent(874/999): loss=[[  7.42219998e+08]]\n",
      "Gradient Descent(875/999): loss=[[ inf]]\n",
      "Gradient Descent(876/999): loss=[[ inf]]\n",
      "Gradient Descent(877/999): loss=[[ inf]]\n",
      "Gradient Descent(878/999): loss=[[  7.04154789e+08]]\n",
      "Gradient Descent(879/999): loss=[[ inf]]\n",
      "Gradient Descent(880/999): loss=[[ inf]]\n",
      "Gradient Descent(881/999): loss=[[ inf]]\n",
      "Gradient Descent(882/999): loss=[[  7.50079711e+08]]\n",
      "Gradient Descent(883/999): loss=[[ inf]]\n",
      "Gradient Descent(884/999): loss=[[ inf]]\n",
      "Gradient Descent(885/999): loss=[[ inf]]\n",
      "Gradient Descent(886/999): loss=[[  7.23127990e+08]]\n",
      "Gradient Descent(887/999): loss=[[ inf]]\n",
      "Gradient Descent(888/999): loss=[[ inf]]\n",
      "Gradient Descent(889/999): loss=[[ inf]]\n",
      "Gradient Descent(890/999): loss=[[  7.36990200e+08]]\n",
      "Gradient Descent(891/999): loss=[[ inf]]\n",
      "Gradient Descent(892/999): loss=[[  3.63330869e+08]]\n",
      "Gradient Descent(893/999): loss=[[ inf]]\n",
      "Gradient Descent(894/999): loss=[[  6.66892999e+08]]\n",
      "Gradient Descent(895/999): loss=[[ inf]]\n",
      "Gradient Descent(896/999): loss=[[ inf]]\n",
      "Gradient Descent(897/999): loss=[[ inf]]\n",
      "Gradient Descent(898/999): loss=[[  7.27961794e+08]]\n",
      "Gradient Descent(899/999): loss=[[ inf]]\n",
      "Gradient Descent(900/999): loss=[[ inf]]\n",
      "Gradient Descent(901/999): loss=[[ inf]]\n",
      "Gradient Descent(902/999): loss=[[  7.69876958e+08]]\n",
      "Gradient Descent(903/999): loss=[[ inf]]\n",
      "Gradient Descent(904/999): loss=[[ inf]]\n",
      "Gradient Descent(905/999): loss=[[ inf]]\n",
      "Gradient Descent(906/999): loss=[[  7.33290218e+08]]\n",
      "Gradient Descent(907/999): loss=[[ inf]]\n",
      "Gradient Descent(908/999): loss=[[ inf]]\n",
      "Gradient Descent(909/999): loss=[[ inf]]\n",
      "Gradient Descent(910/999): loss=[[  7.35614403e+08]]\n",
      "Gradient Descent(911/999): loss=[[ inf]]\n",
      "Gradient Descent(912/999): loss=[[ inf]]\n",
      "Gradient Descent(913/999): loss=[[ inf]]\n",
      "Gradient Descent(914/999): loss=[[  6.85956559e+08]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-58a15ad1f40a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mvalidation_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mallValidationError\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind_l\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\EPFL\\OneNote\\Master\\Ma-3\\Machine Learning\\Project1\\ML_project1\\Notebook\\cross_val.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, gamma, lambda_)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalized_stochastic_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\EPFL\\OneNote\\Master\\Ma-3\\Machine Learning\\Project1\\ML_project1\\Notebook\\logistic_regression.py\u001b[0m in \u001b[0;36mpenalized_stochastic_gradient_descent\u001b[1;34m(y, tx, initial_w, batch_size, max_iter, gamma, lambda_)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatchy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchtx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;31m# store w and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\EPFL\\OneNote\\Master\\Ma-3\\Machine Learning\\Project1\\ML_project1\\Notebook\\logistic_regression.py\u001b[0m in \u001b[0;36mpenalized_logistic_regression\u001b[1;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;34m\"\"\"return the loss, gradient, and hessian.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessian\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;31m# Add the elements due to penalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\EPFL\\OneNote\\Master\\Ma-3\\Machine Learning\\Project1\\ML_project1\\Notebook\\logistic_regression.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mhessian\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcalculate_hessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessian\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\EPFL\\OneNote\\Master\\Ma-3\\Machine Learning\\Project1\\ML_project1\\Notebook\\logistic_regression.py\u001b[0m in \u001b[0;36mcalculate_hessian\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mdia\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "k_fold = 15\n",
    "gammas = np.linspace(0.25,1,4) # Hyperparameter\n",
    "lambdas = np.logspace(-1,8,10) # Hyperparameter\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "allValidationError = np.zeros([len(gammas), len(lambdas)])\n",
    "\n",
    "for ind_g, g in enumerate(gammas):\n",
    "    for ind_l, l in enumerate(lambdas):\n",
    "        validation_error = 0\n",
    "        print(\"gamma: {}\\tlambda: {}\".format(g,l))\n",
    "        for k in range(k_fold):\n",
    "            print(k)\n",
    "            validation_error = validation_error + cross_validation(y, tx, k_indices, k, g, l)\n",
    "        allValidationError[ind_g, ind_l] = validation_error/k_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[[ 13862.9436112]]\n",
      "Current iteration=1, loss=[[ 9565.5228435]]\n",
      "Current iteration=2, loss=[[ 8957.96359879]]\n",
      "Current iteration=3, loss=[[ 8739.83258948]]\n",
      "Current iteration=4, loss=[[ 8645.15851427]]\n",
      "Current iteration=5, loss=[[ 8604.02139854]]\n",
      "Current iteration=6, loss=[[ 8592.5321059]]\n",
      "Current iteration=7, loss=[[ 8592.05446394]]\n",
      "Current iteration=8, loss=[[ 8592.05285207]]\n"
     ]
    }
   ],
   "source": [
    "w,loss=learning_by_penalized_gradient(y, tx, initial_w, max_iter, gamma, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=[[ 4852.03026392]]\n",
      "Gradient Descent(1/99): loss=[[ 3898.78117069]]\n",
      "Gradient Descent(2/99): loss=[[ 3603.57749163]]\n",
      "Gradient Descent(3/99): loss=[[ 3352.98716923]]\n",
      "Gradient Descent(4/99): loss=[[ 3363.00089446]]\n",
      "Gradient Descent(5/99): loss=[[ 3310.79859485]]\n",
      "Gradient Descent(6/99): loss=[[ 3334.30797855]]\n",
      "Gradient Descent(7/99): loss=[[ 3256.72333829]]\n",
      "Gradient Descent(8/99): loss=[[ 3261.99644048]]\n",
      "Gradient Descent(9/99): loss=[[ 3365.34729899]]\n",
      "Gradient Descent(10/99): loss=[[ 3366.32117287]]\n",
      "Gradient Descent(11/99): loss=[[ 3330.98818235]]\n",
      "Gradient Descent(12/99): loss=[[ 3220.28877737]]\n",
      "Gradient Descent(13/99): loss=[[ 3212.35117668]]\n",
      "Gradient Descent(14/99): loss=[[ 3283.37793011]]\n",
      "Gradient Descent(15/99): loss=[[ 3265.47454818]]\n",
      "Gradient Descent(16/99): loss=[[ 3266.01044894]]\n",
      "Gradient Descent(17/99): loss=[[ 3209.82841505]]\n",
      "Gradient Descent(18/99): loss=[[ 3247.23097114]]\n",
      "Gradient Descent(19/99): loss=[[ 3242.4052503]]\n",
      "Gradient Descent(20/99): loss=[[ 3239.82595005]]\n",
      "Gradient Descent(21/99): loss=[[ 3308.63572917]]\n",
      "Gradient Descent(22/99): loss=[[ 3257.17398088]]\n",
      "Gradient Descent(23/99): loss=[[ 3266.48908009]]\n",
      "Gradient Descent(24/99): loss=[[ 3290.18949587]]\n",
      "Gradient Descent(25/99): loss=[[ 3288.31626469]]\n",
      "Gradient Descent(26/99): loss=[[ 3225.82946649]]\n",
      "Gradient Descent(27/99): loss=[[ 3307.73720937]]\n",
      "Gradient Descent(28/99): loss=[[ 3182.94401591]]\n",
      "Gradient Descent(29/99): loss=[[ 3195.44255794]]\n",
      "Gradient Descent(30/99): loss=[[ 3119.02104616]]\n",
      "Gradient Descent(31/99): loss=[[ 3270.85096232]]\n",
      "Gradient Descent(32/99): loss=[[ 3300.15107511]]\n",
      "Gradient Descent(33/99): loss=[[ 3302.03904313]]\n",
      "Gradient Descent(34/99): loss=[[ 3227.60576696]]\n",
      "Gradient Descent(35/99): loss=[[ 3253.41020667]]\n",
      "Gradient Descent(36/99): loss=[[ 3303.77839645]]\n",
      "Gradient Descent(37/99): loss=[[ 3248.26623696]]\n",
      "Gradient Descent(38/99): loss=[[ 3248.78601424]]\n",
      "Gradient Descent(39/99): loss=[[ 3197.23990301]]\n",
      "Gradient Descent(40/99): loss=[[ 3260.50966645]]\n",
      "Gradient Descent(41/99): loss=[[ 3283.34909039]]\n",
      "Gradient Descent(42/99): loss=[[ 3228.1692455]]\n",
      "Gradient Descent(43/99): loss=[[ 3317.64881494]]\n",
      "Gradient Descent(44/99): loss=[[ 3244.61245725]]\n",
      "Gradient Descent(45/99): loss=[[ 3352.47077917]]\n",
      "Gradient Descent(46/99): loss=[[ 3313.18966942]]\n",
      "Gradient Descent(47/99): loss=[[ 3243.10255257]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-8d7c1ac421cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalized_stochastic_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ivan\\Desktop\\Etudes\\GitHub\\ML_project1\\Notebook\\logistic_regression.py\u001b[0m in \u001b[0;36mpenalized_stochastic_gradient_descent\u001b[1;34m(y, tx, initial_w, batch_size, max_iter, gamma, lambda_)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatchy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchtx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ivan\\Desktop\\Etudes\\GitHub\\ML_project1\\Notebook\\helpers.py\u001b[0m in \u001b[0;36mbatch_iter\u001b[1;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mshuffle_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mshuffled_tx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=7000\n",
    "w,loss=penalized_stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iter, gamma, lambda_)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rendering submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_test, features_test = load_data(\"test.csv\")\n",
    "\n",
    "features_train = data_whitening(features_test) # to whiten the data\n",
    "features_train = nonComputableValuesIntegration(features_test)\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_test = build_model_data(features_test) # the \"[1]\" is to take only the \"X\" matrix and not the \"y\"\n",
    "\n",
    "tempData = features_test\n",
    "data = twoFeatureCombinationPower(tempData,1,features_test.shape[1])\n",
    "data = FeaturePower(data,2,features_test.shape[1])\n",
    "\n",
    "\n",
    "y_pred = predict_labels(w1, data[:,:])\n",
    "create_csv_submission(id_test, y_pred, 'first_submission')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
