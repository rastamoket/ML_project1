{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoteBook for the first project\n",
    "- Je pensais que l'on pouvait essayer de faire ca comme ceci:\n",
    "    - Chacun de nous a un block (peut bien entendu en ajouter en dessous du sien) afin de ne pas avoir trop de conflit lorsque l'on git push\n",
    "        - A tester cette semaine pour voir si c'est ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stefan's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from proj1_helpers import * \n",
    "from play_with_data import * # Contains some functions that allow us to look the data and manipulate a little bit\n",
    "from pre_processing import * # Contains all the functions needed to do some pre-processing on the data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********** Import csv file ***********\n",
    "\n",
    "# Import of train.csv --> This was my way of doing it\n",
    "# id_train, labels_train, features_train = load_data(\"train.csv\")\n",
    "\n",
    "# By using the helpers \n",
    "data_path = \"train.csv\"\n",
    "labels_train, features_train, id_train = load_csv_data(data_path)\n",
    "\n",
    "# Import of test.csv --> This is my way of doing it, because it is not done in proj1_helpers\n",
    "id_test, features_test = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Data whitening and \"build model\" ***********\n",
    "# I think the order of the two functions below is important, because if we apply the whitening on the column of 1 it doesn't make sense I thin, do you???????\n",
    "\n",
    "features_train = data_whitening(features_train) # to whiten the data\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_train = build_model_data(features_train, labels_train)[1] # the \"[1]\" is to take only the \"X\" matrix and not the \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bad_ind = np.unique(np.where(features_train == -999.0)[1])\n",
    "features_train = np.delete(features_train,features_bad_ind,1)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(features_train.shape[1]):\n",
    "    for j in list(range(i + 1)):    \n",
    "        newFeature = np.multiply(features_train[:,i],features_train[:,j])\n",
    "        features_train = np.c_[features_train,newFeature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a cross validation with logistic_regression on the data without the features with -999.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters:\n",
    "- Gamma\n",
    "- K --> number of sets for the cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cross_val import *\n",
    "\n",
    "# Prepare the data, remove the features with at least one value at -999.0\n",
    "data_features_removed = remove_features(features_train)\n",
    "\n",
    "seed = 1\n",
    "degree = 5 # No more useful\n",
    "k_fold = 15 # I have seen a decreasing in the error when increasing the number of fold\n",
    "gammas = np.linspace(-0.5,0.5,10)\n",
    "#lambdas = np.logspace(-4, 7, 40)# No longer useful because we don't use anymore the ridge regression\n",
    "\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(labels_train, k_fold, seed)\n",
    "# define lists to store the loss of training data and test data\n",
    "allValidationError = []\n",
    "\n",
    "for g in gammas: # To see with which gamma we obtained a better result\n",
    "    validation_error = 0\n",
    "    for k in range(k_fold):\n",
    "        validation_error = validation_error + cross_validation(labels_train, data_features_removed, k_indices, k, g) \n",
    "    allValidationError.append(validation_error/k_fold) # we compute the mean of the validation error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to have some information about the cross validation we just did\n",
    "plt.close(\"all\")\n",
    "plt.plot(gammas, allValidationError )\n",
    "print(allValidationError)\n",
    "print(min(allValidationError))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FUNCTIONS --> but not so sure of them! Maybe need to re-do (I can put the one I've did from the exercise, I'll do this Friday afternon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To compute the losse, with mse --> with solution\n",
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e)) # Here if we have any problem we should try with a transpose\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "\n",
    "    # compute gradient and loss\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -1/len(y) * np.dot(tx.T, e)\n",
    "    \n",
    "    loss = compute_mse(y,tx,w)\n",
    "    return gradient, loss\n",
    "\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient, loss = compute_gradient(y,tx,w)\n",
    "\n",
    "        # update w by gradient\n",
    "        w = ws[n_iter] - gamma*gradient\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses[-1], ws[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "            \n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -1/len(y) * np.dot(tx.T, e)\n",
    "    \n",
    "    loss = compute_loss(y,tx,w)\n",
    "    return gradient, loss\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    lossForOneRun = 0\n",
    "    \n",
    "    for n_iters in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient, loss = compute_stoch_gradient(minibatch_y, minibatch_tx, ws[n_iters])\n",
    "            lossForOneRun = lossForOneRun + loss       #Faudrait pas reinitialiser avant chaque iteration\n",
    "            w = ws[n_iters] - gamma*gradient\n",
    "        \n",
    "        losses.append((1/batch_size)*lossForOneRun)\n",
    "        ws.append(w)\n",
    "    \n",
    "    return losses[-1], ws[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares --> With solution\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    \n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ridge_regression using normal equations --> with solution\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "\n",
    "    aI = 2 * tx.shape[0] * lambda_*np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    w = np.linalg.solve(a,b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etienne's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"train.csv\")\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "display(trainData.head(5))\n",
    "display(testData.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = trainData.drop(['Id','Prediction'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualization of the data in order to have a first insight of the features and the distribution of non computable(\"NC\")\n",
    "values i.e -999.000\n",
    "'''\n",
    "NCFeatures = []\n",
    "NCValues = []\n",
    "for i in range(trainData.shape[1]):\n",
    "\n",
    "        b = np.where(features_train[:,i] <= -999.000)[0]\n",
    "        if (len(b) >= 1000):\n",
    "            \n",
    "            NCFeatures.append(i)\n",
    "            NCValues.append(len(b))\n",
    "            b = 0\n",
    "            \n",
    "        print('Feature ' + str(i) + ' Histogram')\n",
    "        plt.hist(trainData.iloc[:,i])\n",
    "        plt.show()         \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(NCValues, index = NCFeatures)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivan's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from proj1_helpers import * \n",
    "from play_with_data import * # Contains some functions that allow us to look the data and manipulate a little bit\n",
    "from pre_processing import * # Contains all the functions needed to do some pre-processing on the data\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** Import csv file ***********\n",
    "\n",
    "# Import of train.csv --> This was my way of doing it\n",
    "# id_train, labels_train, features_train = load_data(\"train.csv\")\n",
    "\n",
    "# By using the helpers \n",
    "data_path = \"train.csv\"\n",
    "labels_train, features_train, id_train = load_csv_data(data_path)\n",
    "\n",
    "# Import of test.csv --> This is my way of doing it, because it is not done in proj1_helpers\n",
    "id_test, features_test = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********* Data whitening and \"build model\" ***********\n",
    "# I think the order of the two functions below is important, because if we apply the whitening on the column of 1 it doesn't make sense I thin, do you???????\n",
    "\n",
    "features_train = data_whitening(features_train) # to whiten the data\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_train = build_model_data(features_train, labels_train)[1] # the \"[1]\" is to take only the \"X\" matrix and not the \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 20)\n"
     ]
    }
   ],
   "source": [
    "features_bad_ind = np.unique(np.where(features_train == -999.0)[1])\n",
    "features_train = np.delete(features_train,features_bad_ind,1)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Adding combination of features as new features at the power of choice. Ex (x1x2)^coef, (x1x3)^coef. \n",
    "x1^pow is not calculated here.\n",
    "'''\n",
    "def twoFeatureCombinationPower(data, coef, featNum):\n",
    "    \n",
    "    for i in range(featNum):\n",
    "        for j in list(range(i + 1)):\n",
    "            if i!=j:\n",
    "                newFeature = np.multiply(data[:,i],data[:,j])\n",
    "                for k in range(coef):\n",
    "                    newFeaturePow = np.power(newFeature,k+1)\n",
    "                    data = np.c_[data,newFeaturePow]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Adding combination of features as new features. Ex x1x2, x1x3. x1^2 is not calculated here\n",
    "'''\n",
    "def FeaturePower(data, coef,featNum):\n",
    "    if coef <= 1:\n",
    "        print('No need to use this function you will duplicate features')\n",
    "    else:    \n",
    "        powers = np.linspace(2,coef,coef-1)    \n",
    "        for i in range(featNum):        \n",
    "            newFeature = data[:,i]\n",
    "            for j in powers:\n",
    "                newFeaturePow = np.power(newFeature,j)\n",
    "                data = np.c_[data,newFeaturePow]\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempData = features_train\n",
    "data = twoFeatureCombinationPower(tempData,2,features_train.shape[1])\n",
    "data = FeaturePower(data,2,features_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=labels_train[0:10000]\n",
    "tx=data[0:10000,:]\n",
    "initial_w=np.zeros((tx.shape[1], 1))\n",
    "max_iter=1000\n",
    "gamma=0.7\n",
    "lambda_=0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Etienne/Documents/EPFL/Master/Master 3/ML/Project 1 ML/ML_project1/Notebook/logistic_regression.py:21: RuntimeWarning: overflow encountered in exp\n",
      "  loss_ = np.sum(np.log(1+np.exp(tx.dot(w))))-y.T.dot(tx.dot(w))\n",
      "/Users/Etienne/Documents/EPFL/Master/Master 3/ML/Project 1 ML/ML_project1/Notebook/logistic_regression.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (np.exp(-x) + 1)\n",
      "/Users/Etienne/Documents/EPFL/Master/Master 3/ML/Project 1 ML/ML_project1/Notebook/logistic_regression.py:69: RuntimeWarning: invalid value encountered in subtract\n",
      "  if(len(losses) > 1 and abs(losses[-1] - losses[-2]) <= threshold):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ -2.43298300e+06],\n",
       "        [ -2.16528299e+06],\n",
       "        [  2.59426586e+04],\n",
       "        [  1.19436497e+06],\n",
       "        [  1.68820085e+05],\n",
       "        [ -2.57104717e+05],\n",
       "        [  9.84114145e+05],\n",
       "        [ -1.27669324e+06],\n",
       "        [  1.69478857e+06],\n",
       "        [  1.53847804e+06],\n",
       "        [  6.67827140e+04],\n",
       "        [ -4.82831635e+03],\n",
       "        [ -1.73480682e+05],\n",
       "        [  4.41428958e+04],\n",
       "        [  8.64030172e+04],\n",
       "        [  1.04724369e+05],\n",
       "        [ -3.75775423e+04],\n",
       "        [  8.56439289e+05],\n",
       "        [  7.85320226e+05],\n",
       "        [  8.49000957e+05],\n",
       "        [ -2.16528299e+06],\n",
       "        [ -2.55682690e+06],\n",
       "        [  2.59426586e+04],\n",
       "        [ -5.29223795e+06],\n",
       "        [ -9.93277290e+05],\n",
       "        [ -9.51155168e+06],\n",
       "        [  1.19436497e+06],\n",
       "        [ -6.28831396e+05],\n",
       "        [  7.58912647e+05],\n",
       "        [ -9.53359640e+05],\n",
       "        [ -1.40213188e+05],\n",
       "        [ -5.19542998e+06],\n",
       "        [  1.68820085e+05],\n",
       "        [ -3.54935148e+06],\n",
       "        [ -4.36617209e+05],\n",
       "        [ -3.10873962e+06],\n",
       "        [ -3.38635799e+06],\n",
       "        [ -1.95710347e+07],\n",
       "        [  4.70423444e+03],\n",
       "        [ -3.97959182e+05],\n",
       "        [ -2.57104717e+05],\n",
       "        [ -2.76212478e+06],\n",
       "        [  5.94520760e+05],\n",
       "        [  2.27535586e+05],\n",
       "        [ -6.18292304e+04],\n",
       "        [ -6.17046017e+06],\n",
       "        [ -1.21665414e+06],\n",
       "        [  3.30750238e+06],\n",
       "        [  4.25116244e+05],\n",
       "        [ -4.58561114e+06],\n",
       "        [  9.84114145e+05],\n",
       "        [ -1.86275394e+06],\n",
       "        [  4.31469587e+05],\n",
       "        [ -3.13208863e+06],\n",
       "        [ -6.76235540e+05],\n",
       "        [ -9.04468147e+06],\n",
       "        [ -8.12549583e+05],\n",
       "        [  2.83385468e+06],\n",
       "        [ -5.30421193e+04],\n",
       "        [ -2.32126507e+06],\n",
       "        [ -1.55370409e+06],\n",
       "        [ -1.62124484e+07],\n",
       "        [ -1.27669324e+06],\n",
       "        [ -2.80380703e+06],\n",
       "        [ -9.79984354e+05],\n",
       "        [ -1.06589772e+07],\n",
       "        [ -6.97257023e+05],\n",
       "        [ -1.09891118e+07],\n",
       "        [ -3.57768242e+05],\n",
       "        [ -1.20137223e+07],\n",
       "        [ -4.62384953e+05],\n",
       "        [ -3.56372237e+06],\n",
       "        [ -1.19947231e+05],\n",
       "        [ -8.65789643e+06],\n",
       "        [ -5.34431878e+05],\n",
       "        [ -1.88692329e+07],\n",
       "        [  1.69478857e+06],\n",
       "        [ -1.87296161e+06],\n",
       "        [  1.30302491e+06],\n",
       "        [ -1.94127973e+06],\n",
       "        [  1.57846137e+05],\n",
       "        [ -4.66688143e+06],\n",
       "        [ -7.89871292e+05],\n",
       "        [ -3.10236024e+05],\n",
       "        [ -1.13435521e+05],\n",
       "        [ -2.65640289e+06],\n",
       "        [ -8.85876394e+05],\n",
       "        [ -2.09960474e+06],\n",
       "        [ -7.49324326e+05],\n",
       "        [ -1.45705331e+06],\n",
       "        [  1.04522667e+05],\n",
       "        [ -1.95907879e+06],\n",
       "        [  1.53847804e+06],\n",
       "        [ -7.78504833e+05],\n",
       "        [  9.30985461e+04],\n",
       "        [  6.71099555e+05],\n",
       "        [ -1.05524376e+06],\n",
       "        [ -2.21956739e+07],\n",
       "        [  8.73888324e+03],\n",
       "        [  9.58923212e+06],\n",
       "        [ -3.64511714e+05],\n",
       "        [ -7.26992514e+05],\n",
       "        [ -3.03166107e+05],\n",
       "        [  4.21894403e+06],\n",
       "        [ -3.98144797e+05],\n",
       "        [  2.39559277e+06],\n",
       "        [  5.39349354e+05],\n",
       "        [ -9.81016213e+05],\n",
       "        [ -1.91423550e+05],\n",
       "        [ -8.94932772e+05],\n",
       "        [  6.67827140e+04],\n",
       "        [ -3.33921779e+06],\n",
       "        [ -9.29402090e+04],\n",
       "        [ -3.23444217e+06],\n",
       "        [ -4.48884892e+03],\n",
       "        [ -8.62167628e+06],\n",
       "        [  2.48652305e+03],\n",
       "        [ -1.30560772e+06],\n",
       "        [ -1.42760343e+04],\n",
       "        [ -5.29879179e+06],\n",
       "        [ -2.78185578e+04],\n",
       "        [ -3.74976235e+06],\n",
       "        [  1.78148615e+04],\n",
       "        [ -3.02762293e+06],\n",
       "        [ -8.70119797e+03],\n",
       "        [ -3.64185947e+06],\n",
       "        [  1.25563687e+04],\n",
       "        [ -2.78778158e+06],\n",
       "        [  3.16401980e+04],\n",
       "        [ -1.91500183e+06],\n",
       "        [ -4.82831635e+03],\n",
       "        [ -2.39283007e+06],\n",
       "        [ -1.71281838e+04],\n",
       "        [ -2.37430857e+06],\n",
       "        [ -1.65572537e+04],\n",
       "        [ -5.08927929e+06],\n",
       "        [ -1.16818878e+05],\n",
       "        [ -5.06431902e+05],\n",
       "        [  4.29493471e+04],\n",
       "        [ -3.60612657e+06],\n",
       "        [ -7.49311026e+04],\n",
       "        [ -2.34884432e+06],\n",
       "        [ -8.89836528e+04],\n",
       "        [ -1.68781580e+06],\n",
       "        [ -1.08245404e+05],\n",
       "        [ -2.75142343e+06],\n",
       "        [ -3.15725597e+04],\n",
       "        [ -1.81261727e+06],\n",
       "        [  2.51371323e+04],\n",
       "        [ -9.48492228e+05],\n",
       "        [  2.84438539e+03],\n",
       "        [ -3.32548538e+06],\n",
       "        [ -1.73480682e+05],\n",
       "        [ -2.65403331e+06],\n",
       "        [ -1.06832078e+06],\n",
       "        [ -1.46944595e+07],\n",
       "        [ -1.95885714e+06],\n",
       "        [ -3.10840928e+07],\n",
       "        [ -3.85993638e+05],\n",
       "        [ -5.69404477e+06],\n",
       "        [ -8.23635483e+05],\n",
       "        [ -1.54996614e+06],\n",
       "        [ -3.79856172e+05],\n",
       "        [ -7.89460949e+06],\n",
       "        [ -9.99289298e+05],\n",
       "        [ -1.70617544e+07],\n",
       "        [ -2.02768217e+06],\n",
       "        [ -7.23242497e+07],\n",
       "        [ -2.34693051e+04],\n",
       "        [ -2.07984776e+06],\n",
       "        [ -4.72015678e+05],\n",
       "        [  1.71603152e+06],\n",
       "        [  3.25816765e+04],\n",
       "        [ -3.61088505e+06],\n",
       "        [ -5.16405228e+04],\n",
       "        [ -2.71583986e+06],\n",
       "        [  4.41428958e+04],\n",
       "        [ -3.41354712e+06],\n",
       "        [ -3.83361591e+04],\n",
       "        [ -3.21097259e+06],\n",
       "        [ -1.79954942e+03],\n",
       "        [ -8.42157790e+06],\n",
       "        [  4.27795351e+04],\n",
       "        [ -1.51633248e+06],\n",
       "        [ -4.20193653e+04],\n",
       "        [ -5.22859352e+06],\n",
       "        [ -6.50909956e+04],\n",
       "        [ -3.75166718e+06],\n",
       "        [  5.21052415e+04],\n",
       "        [ -2.80556167e+06],\n",
       "        [ -1.14652092e+04],\n",
       "        [ -3.33283668e+06],\n",
       "        [ -1.96928721e+04],\n",
       "        [ -2.83420133e+06],\n",
       "        [  6.53057775e+04],\n",
       "        [ -1.88587958e+06],\n",
       "        [ -1.49512459e+06],\n",
       "        [ -4.81689970e+06],\n",
       "        [  8.22218694e+04],\n",
       "        [ -3.36452214e+06],\n",
       "        [  3.50051552e+04],\n",
       "        [ -2.94720776e+06],\n",
       "        [  8.64030172e+04],\n",
       "        [ -2.41341173e+06],\n",
       "        [ -3.67779826e+04],\n",
       "        [ -2.57865278e+06],\n",
       "        [  3.78160532e+04],\n",
       "        [ -5.44318698e+06],\n",
       "        [  7.07133695e+03],\n",
       "        [ -3.34198685e+05],\n",
       "        [  2.04802932e+04],\n",
       "        [ -3.56420099e+06],\n",
       "        [  1.66205310e+04],\n",
       "        [ -2.71822218e+06],\n",
       "        [ -1.06488745e+03],\n",
       "        [ -1.67561830e+06],\n",
       "        [  1.83945812e+03],\n",
       "        [ -2.99127341e+06],\n",
       "        [  7.67324458e+04],\n",
       "        [ -1.90263803e+06],\n",
       "        [ -1.32935678e+04],\n",
       "        [ -5.93938873e+05],\n",
       "        [ -3.06299262e+04],\n",
       "        [ -3.22041864e+06],\n",
       "        [  2.45856851e+05],\n",
       "        [ -2.11598061e+06],\n",
       "        [ -6.84046403e+04],\n",
       "        [ -2.78599156e+06],\n",
       "        [  3.39917376e+04],\n",
       "        [ -3.38359738e+06],\n",
       "        [  1.04724369e+05],\n",
       "        [ -7.94697108e+05],\n",
       "        [ -6.24634820e+05],\n",
       "        [  3.75959612e+05],\n",
       "        [ -2.21357913e+05],\n",
       "        [ -4.39595471e+06],\n",
       "        [ -7.99172262e+03],\n",
       "        [  4.38630276e+06],\n",
       "        [ -5.04345382e+04],\n",
       "        [ -1.30210571e+06],\n",
       "        [ -7.22318710e+05],\n",
       "        [  2.76981814e+07],\n",
       "        [ -1.97488437e+05],\n",
       "        [ -5.29684885e+06],\n",
       "        [  4.34509327e+03],\n",
       "        [ -7.21182246e+06],\n",
       "        [  2.28586704e+05],\n",
       "        [ -2.26055896e+05],\n",
       "        [ -3.63133195e+04],\n",
       "        [  4.81785472e+06],\n",
       "        [ -3.28686993e+04],\n",
       "        [ -1.13420003e+06],\n",
       "        [ -6.32792812e+04],\n",
       "        [ -5.00828756e+05],\n",
       "        [ -4.51905364e+04],\n",
       "        [ -2.53766121e+06],\n",
       "        [  5.26050951e+04],\n",
       "        [ -1.50687226e+06],\n",
       "        [  3.64198920e+04],\n",
       "        [ -3.06771065e+05],\n",
       "        [ -3.75775423e+04],\n",
       "        [ -2.44387835e+06],\n",
       "        [  7.55232291e+03],\n",
       "        [ -2.55818200e+06],\n",
       "        [ -1.91065152e+04],\n",
       "        [ -5.57029717e+06],\n",
       "        [  3.70977979e+03],\n",
       "        [ -5.11630796e+05],\n",
       "        [ -1.09398670e+04],\n",
       "        [ -3.54720999e+06],\n",
       "        [ -9.53144825e+04],\n",
       "        [ -2.75202304e+06],\n",
       "        [  2.40837191e+04],\n",
       "        [ -1.74907748e+06],\n",
       "        [ -5.14216967e+03],\n",
       "        [ -2.72869890e+06],\n",
       "        [  2.56660780e+04],\n",
       "        [ -1.89003424e+06],\n",
       "        [  1.03839408e+05],\n",
       "        [ -8.27559582e+05],\n",
       "        [  1.63539095e+05],\n",
       "        [ -3.35590718e+06],\n",
       "        [ -8.94855962e+04],\n",
       "        [ -2.34819160e+06],\n",
       "        [  1.14668012e+04],\n",
       "        [ -2.81455992e+06],\n",
       "        [  7.00705304e+04],\n",
       "        [ -3.36098219e+06],\n",
       "        [  1.11690932e+06],\n",
       "        [ -1.03712150e+06],\n",
       "        [ -6.36993712e+03],\n",
       "        [ -4.12861208e+05],\n",
       "        [  8.56439289e+05],\n",
       "        [ -2.30123934e+06],\n",
       "        [  5.82886049e+05],\n",
       "        [ -3.74292764e+06],\n",
       "        [ -5.36487740e+05],\n",
       "        [ -9.23394900e+06],\n",
       "        [ -9.84674847e+05],\n",
       "        [  1.42945359e+06],\n",
       "        [  1.12271864e+05],\n",
       "        [ -3.04139246e+06],\n",
       "        [ -1.78842235e+06],\n",
       "        [ -1.38120133e+07],\n",
       "        [ -1.83090429e+06],\n",
       "        [ -1.31405576e+07],\n",
       "        [ -3.38547679e+05],\n",
       "        [ -1.18808999e+07],\n",
       "        [ -8.94156703e+05],\n",
       "        [ -1.95735263e+06],\n",
       "        [ -4.28690623e+05],\n",
       "        [  1.34843853e+06],\n",
       "        [  6.65443139e+04],\n",
       "        [ -3.38343793e+06],\n",
       "        [ -7.25815445e+04],\n",
       "        [ -2.07872808e+06],\n",
       "        [ -8.26801667e+05],\n",
       "        [ -1.28574382e+07],\n",
       "        [  7.25480783e+04],\n",
       "        [ -3.17739986e+06],\n",
       "        [ -2.22727645e+04],\n",
       "        [ -2.21740792e+06],\n",
       "        [ -3.07067714e+05],\n",
       "        [ -5.24397143e+06],\n",
       "        [  3.59334071e+04],\n",
       "        [ -2.28865990e+06],\n",
       "        [  7.85320226e+05],\n",
       "        [ -2.71582517e+06],\n",
       "        [  8.14516794e+05],\n",
       "        [ -3.17912161e+06],\n",
       "        [ -8.47791205e+04],\n",
       "        [ -5.54694976e+06],\n",
       "        [ -1.20083979e+06],\n",
       "        [ -2.11773621e+06],\n",
       "        [  4.06025977e+05],\n",
       "        [ -4.10089866e+06],\n",
       "        [ -1.55082795e+06],\n",
       "        [ -7.26035512e+06],\n",
       "        [ -1.92727263e+06],\n",
       "        [ -5.64506729e+06],\n",
       "        [ -1.55022282e+05],\n",
       "        [ -5.24850845e+06],\n",
       "        [ -1.00213582e+06],\n",
       "        [ -2.23241917e+06],\n",
       "        [ -4.02893388e+05],\n",
       "        [ -1.15700745e+06],\n",
       "        [  1.39311815e+04],\n",
       "        [ -3.55959984e+06],\n",
       "        [ -1.18904148e+05],\n",
       "        [ -2.68915273e+06],\n",
       "        [ -5.06305306e+05],\n",
       "        [ -4.97359628e+06],\n",
       "        [ -4.43531340e+04],\n",
       "        [ -3.69089796e+06],\n",
       "        [  1.53063521e+04],\n",
       "        [ -2.73698083e+06],\n",
       "        [ -2.80615916e+05],\n",
       "        [ -2.58906313e+06],\n",
       "        [ -6.75504120e+03],\n",
       "        [ -2.73640534e+06],\n",
       "        [ -1.98678502e+06],\n",
       "        [ -5.79698214e+06],\n",
       "        [  8.49000957e+05],\n",
       "        [ -2.07357531e+06],\n",
       "        [  7.28552342e+05],\n",
       "        [ -3.19789765e+06],\n",
       "        [ -1.16025502e+05],\n",
       "        [ -6.45636462e+06],\n",
       "        [ -8.74307819e+05],\n",
       "        [ -8.84629624e+05],\n",
       "        [  2.06146901e+05],\n",
       "        [ -3.32228641e+06],\n",
       "        [ -1.67929246e+06],\n",
       "        [ -2.15694783e+07],\n",
       "        [ -1.88295911e+06],\n",
       "        [ -2.07936525e+07],\n",
       "        [ -2.97751927e+05],\n",
       "        [ -1.29094351e+07],\n",
       "        [ -8.35512785e+05],\n",
       "        [ -1.56752848e+06],\n",
       "        [ -1.85733988e+05],\n",
       "        [  2.39254399e+06],\n",
       "        [  6.46062463e+03],\n",
       "        [ -3.19875663e+06],\n",
       "        [ -9.91660257e+04],\n",
       "        [ -1.77978464e+06],\n",
       "        [ -4.74250548e+05],\n",
       "        [ -9.13933114e+06],\n",
       "        [  3.86968714e+04],\n",
       "        [ -3.00924277e+06],\n",
       "        [  1.71813158e+04],\n",
       "        [ -1.86711243e+06],\n",
       "        [ -2.14655424e+05],\n",
       "        [ -8.69954124e+06],\n",
       "        [  2.10523886e+03],\n",
       "        [ -1.90966633e+06],\n",
       "        [ -1.87720659e+06],\n",
       "        [ -1.73964637e+07],\n",
       "        [ -2.06901569e+06],\n",
       "        [ -6.72131265e+06],\n",
       "        [ -2.43298300e+06],\n",
       "        [ -2.55682690e+06],\n",
       "        [ -5.29223795e+06],\n",
       "        [ -6.28831396e+05],\n",
       "        [ -3.54935148e+06],\n",
       "        [ -2.76212478e+06],\n",
       "        [ -1.86275394e+06],\n",
       "        [ -2.80380703e+06],\n",
       "        [ -1.87296161e+06],\n",
       "        [ -7.78504833e+05],\n",
       "        [ -3.33921779e+06],\n",
       "        [ -2.39283007e+06],\n",
       "        [ -2.65403331e+06],\n",
       "        [ -3.41354712e+06],\n",
       "        [ -2.41341173e+06],\n",
       "        [ -7.94697108e+05],\n",
       "        [ -2.44387835e+06],\n",
       "        [ -2.30123934e+06],\n",
       "        [ -2.71582517e+06],\n",
       "        [ -2.07357531e+06]]), array([ inf]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from logistic_regression import *\n",
    "ws, losses = learning_grad_descent(y, tx, initial_w, max_iter, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
