{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoteBook for the first project\n",
    "- Je pensais que l'on pouvait essayer de faire ca comme ceci:\n",
    "    - Chacun de nous a un block (peut bien entendu en ajouter en dessous du sien) afin de ne pas avoir trop de conflit lorsque l'on git push\n",
    "        - A tester cette semaine pour voir si c'est ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stefan's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from proj1_helpers import * \n",
    "from play_with_data import * # Contains some functions that allow us to look the data and manipulate a little bit\n",
    "from pre_processing import * # Contains all the functions needed to do some pre-processing on the data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********** Import csv file ***********\n",
    "\n",
    "# Import of train.csv --> This was my way of doing it\n",
    "# id_train, labels_train, features_train = load_data(\"train.csv\")\n",
    "\n",
    "# By using the helpers \n",
    "data_path = \"train.csv\"\n",
    "labels_train, features_train, id_train = load_csv_data(data_path)\n",
    "\n",
    "# Import of test.csv --> This is my way of doing it, because it is not done in proj1_helpers\n",
    "id_test, features_test = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Data whitening and \"build model\" ***********\n",
    "# I think the order of the two functions below is important, because if we apply the whitening on the column of 1 it doesn't make sense I thin, do you???????\n",
    "\n",
    "features_train = data_whitening(features_train) # to whiten the data\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_train = build_model_data(features_train, labels_train)[1] # the \"[1]\" is to take only the \"X\" matrix and not the \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_bad_ind = np.unique(np.where(features_train == -999.0)[1])\n",
    "features_train = np.delete(features_train,features_bad_ind,1)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(features_train.shape[1]):\n",
    "    for j in list(range(i + 1)):    \n",
    "        newFeature = np.multiply(features_train[:,i],features_train[:,j])\n",
    "        features_train = np.c_[features_train,newFeature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a cross validation with logistic_regression on the data without the features with -999.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters:\n",
    "- Gamma\n",
    "- K --> number of sets for the cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cross_val import *\n",
    "\n",
    "# Prepare the data, remove the features with at least one value at -999.0\n",
    "data_features_removed = remove_features(features_train)\n",
    "\n",
    "seed = 1\n",
    "degree = 5 # No more useful\n",
    "k_fold = 15 # I have seen a decreasing in the error when increasing the number of fold\n",
    "gammas = np.linspace(-0.5,0.5,10)\n",
    "#lambdas = np.logspace(-4, 7, 40)# No longer useful because we don't use anymore the ridge regression\n",
    "\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(labels_train, k_fold, seed)\n",
    "# define lists to store the loss of training data and test data\n",
    "allValidationError = []\n",
    "\n",
    "for g in gammas: # To see with which gamma we obtained a better result\n",
    "    validation_error = 0\n",
    "    for k in range(k_fold):\n",
    "        validation_error = validation_error + cross_validation(labels_train, data_features_removed, k_indices, k, g) \n",
    "    allValidationError.append(validation_error/k_fold) # we compute the mean of the validation error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to have some information about the cross validation we just did\n",
    "plt.close(\"all\")\n",
    "plt.plot(gammas, allValidationError )\n",
    "print(allValidationError)\n",
    "print(min(allValidationError))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FUNCTIONS --> but not so sure of them! Maybe need to re-do (I can put the one I've did from the exercise, I'll do this Friday afternon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To compute the losse, with mse --> with solution\n",
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e)) # Here if we have any problem we should try with a transpose\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "\n",
    "    # compute gradient and loss\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -1/len(y) * np.dot(tx.T, e)\n",
    "    \n",
    "    loss = compute_mse(y,tx,w)\n",
    "    return gradient, loss\n",
    "\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient, loss = compute_gradient(y,tx,w)\n",
    "\n",
    "        # update w by gradient\n",
    "        w = ws[n_iter] - gamma*gradient\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses[-1], ws[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "            \n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -1/len(y) * np.dot(tx.T, e)\n",
    "    \n",
    "    loss = compute_loss(y,tx,w)\n",
    "    return gradient, loss\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    lossForOneRun = 0\n",
    "    \n",
    "    for n_iters in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient, loss = compute_stoch_gradient(minibatch_y, minibatch_tx, ws[n_iters])\n",
    "            lossForOneRun = lossForOneRun + loss       #Faudrait pas reinitialiser avant chaque iteration\n",
    "            w = ws[n_iters] - gamma*gradient\n",
    "        \n",
    "        losses.append((1/batch_size)*lossForOneRun)\n",
    "        ws.append(w)\n",
    "    \n",
    "    return losses[-1], ws[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares --> With solution\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    \n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ridge_regression using normal equations --> with solution\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "\n",
    "    aI = 2 * tx.shape[0] * lambda_*np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    w = np.linalg.solve(a,b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etienne's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"train.csv\")\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "display(trainData.head(5))\n",
    "display(testData.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = trainData.drop(['Id','Prediction'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Visualization of the data in order to have a first insight of the features and the distribution of non computable(\"NC\")\n",
    "values i.e -999.000\n",
    "'''\n",
    "NCFeatures = []\n",
    "NCValues = []\n",
    "for i in range(trainData.shape[1]):\n",
    "\n",
    "        b = np.where(features_train[:,i] <= -999.000)[0]\n",
    "        if (len(b) >= 1000):\n",
    "            \n",
    "            NCFeatures.append(i)\n",
    "            NCValues.append(len(b))\n",
    "            b = 0\n",
    "            \n",
    "        print('Feature ' + str(i) + ' Histogram')\n",
    "        plt.hist(trainData.iloc[:,i])\n",
    "        plt.show()         \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(NCValues, index = NCFeatures)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivan's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from proj1_helpers import * \n",
    "from play_with_data import * # Contains some functions that allow us to look the data and manipulate a little bit\n",
    "from pre_processing import * # Contains all the functions needed to do some pre-processing on the data\n",
    "from logistic_regression import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** Import csv file ***********\n",
    "\n",
    "# Import of train.csv --> This was my way of doing it\n",
    "# id_train, labels_train, features_train = load_data(\"train.csv\")\n",
    "\n",
    "# By using the helpers \n",
    "data_path = \"train.csv\"\n",
    "labels_train, features_train, id_train = load_csv_data(data_path)\n",
    "labels_train[np.where(labels_train == -1)[0]] = 0\n",
    "\n",
    "# Import of test.csv --> This is my way of doing it, because it is not done in proj1_helpers\n",
    "#id_test, features_test = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Data whitening and \"build model\" ***********\n",
    "# I think the order of the two functions below is important, because if we apply the whitening on the column of 1 it doesn't make sense I thin, do you???????\n",
    "\n",
    "features_train = data_whitening(features_train) # to whiten the data\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_train = build_model_data(features_train, labels_train)[1] # the \"[1]\" is to take only the \"X\" matrix and not the \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 20)\n"
     ]
    }
   ],
   "source": [
    "features_bad_ind = np.unique(np.where(features_train == -999.0)[1])\n",
    "features_train = np.delete(features_train,features_bad_ind,1)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Adding combination of features as new features at the power of choice. Ex (x1x2)^coef, (x1x3)^coef. \n",
    "x1^pow is not calculated here.\n",
    "'''\n",
    "def twoFeatureCombinationPower(data, coef, featNum):\n",
    "    \n",
    "    for i in range(featNum):\n",
    "        for j in list(range(i + 1)):\n",
    "            if i!=j:\n",
    "                newFeature = np.multiply(data[:,i],data[:,j])\n",
    "                for k in range(coef):\n",
    "                    newFeaturePow = np.power(newFeature,k+1)\n",
    "                    data = np.c_[data,newFeaturePow]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Adding combination of features as new features. Ex x1^2 is calculated here\n",
    "'''\n",
    "def FeaturePower(data, coef,featNum):\n",
    "    if coef <= 1:\n",
    "        print('No need to use this function you will duplicate features')\n",
    "    else:    \n",
    "        powers = np.linspace(2,coef,coef-1)    \n",
    "        for i in range(featNum):        \n",
    "            newFeature = data[:,i]\n",
    "            for j in powers:\n",
    "                newFeaturePow = np.power(newFeature,j)\n",
    "                data = np.c_[data,newFeaturePow]\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 210)\n",
      "(250000, 230)\n"
     ]
    }
   ],
   "source": [
    "tempData = features_train\n",
    "data = twoFeatureCombinationPower(tempData,1,features_train.shape[1])\n",
    "print(data.shape)\n",
    "data = FeaturePower(data,2,features_train.shape[1])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=labels_train[110000:130000]\n",
    "tx=data[110000:130000,:]\n",
    "initial_w=np.zeros((tx.shape[1], 1))\n",
    "max_iter=100\n",
    "gamma=1\n",
    "lambda_=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "k_fold = 15\n",
    "gammas = np.linspace(0.25,1,4) # Hyperparameter\n",
    "lambdas = np.logspace(-1,8,10) # Hyperparameter\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "allValidationError = np.zeros([len(gammas), len(lambdas)])\n",
    "\n",
    "for ind_g, g in enumerate(gammas):\n",
    "    for ind_l, l in enumerate(lambdas):\n",
    "        validation_error = 0\n",
    "        print(\"gamma: {}\\tlambda: {}\".format(g,l))\n",
    "        for k in range(k_fold):\n",
    "            print(k)\n",
    "            validation_error = validation_error + cross_validation(y, tx, k_indices, k, g, l)\n",
    "        allValidationError[ind_g, ind_l] = validation_error/k_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=[[ 13862.9436112]]\n",
      "Current iteration=1, loss=[[ 9565.5228435]]\n",
      "Current iteration=2, loss=[[ 8957.96359879]]\n",
      "Current iteration=3, loss=[[ 8739.83258948]]\n",
      "Current iteration=4, loss=[[ 8645.15851427]]\n",
      "Current iteration=5, loss=[[ 8604.02139854]]\n",
      "Current iteration=6, loss=[[ 8592.5321059]]\n",
      "Current iteration=7, loss=[[ 8592.05446394]]\n",
      "Current iteration=8, loss=[[ 8592.05285207]]\n"
     ]
    }
   ],
   "source": [
    "w,loss=learning_by_penalized_gradient(y, tx, initial_w, max_iter, gamma, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=[[ 4852.03026392]]\n",
      "Gradient Descent(1/99): loss=[[ 3898.78117069]]\n",
      "Gradient Descent(2/99): loss=[[ 3603.57749163]]\n",
      "Gradient Descent(3/99): loss=[[ 3352.98716923]]\n",
      "Gradient Descent(4/99): loss=[[ 3363.00089446]]\n",
      "Gradient Descent(5/99): loss=[[ 3310.79859485]]\n",
      "Gradient Descent(6/99): loss=[[ 3334.30797855]]\n",
      "Gradient Descent(7/99): loss=[[ 3256.72333829]]\n",
      "Gradient Descent(8/99): loss=[[ 3261.99644048]]\n",
      "Gradient Descent(9/99): loss=[[ 3365.34729899]]\n",
      "Gradient Descent(10/99): loss=[[ 3366.32117287]]\n",
      "Gradient Descent(11/99): loss=[[ 3330.98818235]]\n",
      "Gradient Descent(12/99): loss=[[ 3220.28877737]]\n",
      "Gradient Descent(13/99): loss=[[ 3212.35117668]]\n",
      "Gradient Descent(14/99): loss=[[ 3283.37793011]]\n",
      "Gradient Descent(15/99): loss=[[ 3265.47454818]]\n",
      "Gradient Descent(16/99): loss=[[ 3266.01044894]]\n",
      "Gradient Descent(17/99): loss=[[ 3209.82841505]]\n",
      "Gradient Descent(18/99): loss=[[ 3247.23097114]]\n",
      "Gradient Descent(19/99): loss=[[ 3242.4052503]]\n",
      "Gradient Descent(20/99): loss=[[ 3239.82595005]]\n",
      "Gradient Descent(21/99): loss=[[ 3308.63572917]]\n",
      "Gradient Descent(22/99): loss=[[ 3257.17398088]]\n",
      "Gradient Descent(23/99): loss=[[ 3266.48908009]]\n",
      "Gradient Descent(24/99): loss=[[ 3290.18949587]]\n",
      "Gradient Descent(25/99): loss=[[ 3288.31626469]]\n",
      "Gradient Descent(26/99): loss=[[ 3225.82946649]]\n",
      "Gradient Descent(27/99): loss=[[ 3307.73720937]]\n",
      "Gradient Descent(28/99): loss=[[ 3182.94401591]]\n",
      "Gradient Descent(29/99): loss=[[ 3195.44255794]]\n",
      "Gradient Descent(30/99): loss=[[ 3119.02104616]]\n",
      "Gradient Descent(31/99): loss=[[ 3270.85096232]]\n",
      "Gradient Descent(32/99): loss=[[ 3300.15107511]]\n",
      "Gradient Descent(33/99): loss=[[ 3302.03904313]]\n",
      "Gradient Descent(34/99): loss=[[ 3227.60576696]]\n",
      "Gradient Descent(35/99): loss=[[ 3253.41020667]]\n",
      "Gradient Descent(36/99): loss=[[ 3303.77839645]]\n",
      "Gradient Descent(37/99): loss=[[ 3248.26623696]]\n",
      "Gradient Descent(38/99): loss=[[ 3248.78601424]]\n",
      "Gradient Descent(39/99): loss=[[ 3197.23990301]]\n",
      "Gradient Descent(40/99): loss=[[ 3260.50966645]]\n",
      "Gradient Descent(41/99): loss=[[ 3283.34909039]]\n",
      "Gradient Descent(42/99): loss=[[ 3228.1692455]]\n",
      "Gradient Descent(43/99): loss=[[ 3317.64881494]]\n",
      "Gradient Descent(44/99): loss=[[ 3244.61245725]]\n",
      "Gradient Descent(45/99): loss=[[ 3352.47077917]]\n",
      "Gradient Descent(46/99): loss=[[ 3313.18966942]]\n",
      "Gradient Descent(47/99): loss=[[ 3243.10255257]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-8d7c1ac421cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalized_stochastic_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ivan\\Desktop\\Etudes\\GitHub\\ML_project1\\Notebook\\logistic_regression.py\u001b[0m in \u001b[0;36mpenalized_stochastic_gradient_descent\u001b[1;34m(y, tx, initial_w, batch_size, max_iter, gamma, lambda_)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatchy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchtx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ivan\\Desktop\\Etudes\\GitHub\\ML_project1\\Notebook\\helpers.py\u001b[0m in \u001b[0;36mbatch_iter\u001b[1;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mshuffle_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mshuffled_tx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=7000\n",
    "w,loss=penalized_stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iter, gamma, lambda_)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rendering submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_test, features_test = load_data(\"test.csv\")\n",
    "\n",
    "features_train = data_whitening(features_test) # to whiten the data\n",
    "features_train = nonComputableValuesIntegration(features_test)\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_test = build_model_data(features_test) # the \"[1]\" is to take only the \"X\" matrix and not the \"y\"\n",
    "\n",
    "tempData = features_test\n",
    "data = twoFeatureCombinationPower(tempData,1,features_test.shape[1])\n",
    "data = FeaturePower(data,2,features_test.shape[1])\n",
    "\n",
    "\n",
    "y_pred = predict_labels(w1, data[:,:])\n",
    "create_csv_submission(id_test, y_pred, 'first_submission')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
