{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoteBook for the first project\n",
    "- Je pensais que l'on pouvait essayer de faire ca comme ceci:\n",
    "    - Chacun de nous a un block (peut bien entendu en ajouter en dessous du sien) afin de ne pas avoir trop de conflit lorsque l'on git push\n",
    "        - A tester cette semaine pour voir si c'est ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stefan's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from proj1_helpers import * \n",
    "from play_with_data import * # Contains some functions that allow us to look the data and manipulate a little bit\n",
    "from pre_processing import * # Contains all the functions needed to do some pre-processing on the data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********** Import csv file ***********\n",
    "\n",
    "# Import of train.csv --> This was my way of doing it\n",
    "# id_train, labels_train, features_train = load_data(\"train.csv\")\n",
    "\n",
    "# By using the helpers \n",
    "data_path = \"train.csv\"\n",
    "labels_train, features_train, id_train = load_csv_data(data_path)\n",
    "\n",
    "# Import of test.csv --> This is my way of doing it, because it is not done in proj1_helpers\n",
    "id_test, features_test = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Data whitening and \"build model\" ***********\n",
    "# I think the order of the two functions below is important, because if we apply the whitening on the column of 1 it doesn't make sense I thin, do you???????\n",
    "\n",
    "features_train = data_whitening(features_train) # to whiten the data\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_train = build_model_data(features_train, labels_train)[1] # the \"[1]\" is to take only the \"X\" matrix and not the \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_bad_ind = np.unique(np.where(features_train == -999.0)[1])\n",
    "features_train = np.delete(features_train,features_bad_ind,1)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(features_train.shape[1]):\n",
    "    for j in list(range(i + 1)):    \n",
    "        newFeature = np.multiply(features_train[:,i],features_train[:,j])\n",
    "        features_train = np.c_[features_train,newFeature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a cross validation with logistic_regression on the data without the features with -999.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters:\n",
    "- Gamma\n",
    "- K --> number of sets for the cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cross_val import *\n",
    "\n",
    "# Prepare the data, remove the features with at least one value at -999.0\n",
    "data_features_removed = remove_features(features_train)\n",
    "\n",
    "seed = 1\n",
    "degree = 5 # No more useful\n",
    "k_fold = 15 # I have seen a decreasing in the error when increasing the number of fold\n",
    "gammas = np.linspace(-0.5,0.5,10)\n",
    "#lambdas = np.logspace(-4, 7, 40)# No longer useful because we don't use anymore the ridge regression\n",
    "\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(labels_train, k_fold, seed)\n",
    "# define lists to store the loss of training data and test data\n",
    "allValidationError = []\n",
    "\n",
    "for g in gammas: # To see with which gamma we obtained a better result\n",
    "    validation_error = 0\n",
    "    for k in range(k_fold):\n",
    "        validation_error = validation_error + cross_validation(labels_train, data_features_removed, k_indices, k, g) \n",
    "    allValidationError.append(validation_error/k_fold) # we compute the mean of the validation error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to have some information about the cross validation we just did\n",
    "plt.close(\"all\")\n",
    "plt.plot(gammas, allValidationError )\n",
    "print(allValidationError)\n",
    "print(min(allValidationError))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FUNCTIONS --> but not so sure of them! Maybe need to re-do (I can put the one I've did from the exercise, I'll do this Friday afternon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To compute the losse, with mse --> with solution\n",
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e)) # Here if we have any problem we should try with a transpose\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "\n",
    "    # compute gradient and loss\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -1/len(y) * np.dot(tx.T, e)\n",
    "    \n",
    "    loss = compute_mse(y,tx,w)\n",
    "    return gradient, loss\n",
    "\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient, loss = compute_gradient(y,tx,w)\n",
    "\n",
    "        # update w by gradient\n",
    "        w = ws[n_iter] - gamma*gradient\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses[-1], ws[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "            \n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -1/len(y) * np.dot(tx.T, e)\n",
    "    \n",
    "    loss = compute_loss(y,tx,w)\n",
    "    return gradient, loss\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    lossForOneRun = 0\n",
    "    \n",
    "    for n_iters in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient, loss = compute_stoch_gradient(minibatch_y, minibatch_tx, ws[n_iters])\n",
    "            lossForOneRun = lossForOneRun + loss       #Faudrait pas reinitialiser avant chaque iteration\n",
    "            w = ws[n_iters] - gamma*gradient\n",
    "        \n",
    "        losses.append((1/batch_size)*lossForOneRun)\n",
    "        ws.append(w)\n",
    "    \n",
    "    return losses[-1], ws[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares --> With solution\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    \n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ridge_regression using normal equations --> with solution\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "\n",
    "    aI = 2 * tx.shape[0] * lambda_*np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    w = np.linalg.solve(a,b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etienne's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"train.csv\")\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "display(trainData.head(5))\n",
    "display(testData.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = trainData.drop(['Id','Prediction'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Visualization of the data in order to have a first insight of the features and the distribution of non computable(\"NC\")\n",
    "values i.e -999.000\n",
    "'''\n",
    "NCFeatures = []\n",
    "NCValues = []\n",
    "for i in range(trainData.shape[1]):\n",
    "\n",
    "        b = np.where(features_train[:,i] <= -999.000)[0]\n",
    "        if (len(b) >= 1000):\n",
    "            \n",
    "            NCFeatures.append(i)\n",
    "            NCValues.append(len(b))\n",
    "            b = 0\n",
    "            \n",
    "        print('Feature ' + str(i) + ' Histogram')\n",
    "        plt.hist(trainData.iloc[:,i])\n",
    "        plt.show()         \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(NCValues, index = NCFeatures)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivan's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from proj1_helpers import * \n",
    "from play_with_data import * # Contains some functions that allow us to look the data and manipulate a little bit\n",
    "from pre_processing import * # Contains all the functions needed to do some pre-processing on the data\n",
    "from logistic_regression import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********** Import csv file ***********\n",
    "\n",
    "# Import of train.csv --> This was my way of doing it\n",
    "# id_train, labels_train, features_train = load_data(\"train.csv\")\n",
    "\n",
    "# By using the helpers \n",
    "data_path = \"train.csv\"\n",
    "labels_train, features_train, id_train = load_csv_data(data_path)\n",
    "\n",
    "# Import of test.csv --> This is my way of doing it, because it is not done in proj1_helpers\n",
    "id_test, features_test = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Data whitening and \"build model\" ***********\n",
    "# I think the order of the two functions below is important, because if we apply the whitening on the column of 1 it doesn't make sense I thin, do you???????\n",
    "\n",
    "features_train = data_whitening(features_train) # to whiten the data\n",
    "# Create the data for the model --> add a column of \"1\" as the first column --> this is for the w0 (offset parameter)\n",
    "features_train = build_model_data(features_train, labels_train)[1] # the \"[1]\" is to take only the \"X\" matrix and not the \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 20)\n"
     ]
    }
   ],
   "source": [
    "features_bad_ind = np.unique(np.where(features_train == -999.0)[1])\n",
    "features_train = np.delete(features_train,features_bad_ind,1)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Adding combination of features as new features at the power of choice. Ex (x1x2)^coef, (x1x3)^coef. \n",
    "x1^pow is not calculated here.\n",
    "'''\n",
    "def twoFeatureCombinationPower(data, coef, featNum):\n",
    "    \n",
    "    for i in range(featNum):\n",
    "        for j in list(range(i + 1)):\n",
    "            if i!=j:\n",
    "                newFeature = np.multiply(data[:,i],data[:,j])\n",
    "                for k in range(coef):\n",
    "                    newFeaturePow = np.power(newFeature,k+1)\n",
    "                    data = np.c_[data,newFeaturePow]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Adding combination of features as new features. Ex x1^2 is calculated here\n",
    "'''\n",
    "def FeaturePower(data, coef,featNum):\n",
    "    if coef <= 1:\n",
    "        print('No need to use this function you will duplicate features')\n",
    "    else:    \n",
    "        powers = np.linspace(2,coef,coef-1)    \n",
    "        for i in range(featNum):        \n",
    "            newFeature = data[:,i]\n",
    "            for j in powers:\n",
    "                newFeaturePow = np.power(newFeature,j)\n",
    "                data = np.c_[data,newFeaturePow]\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 210)\n",
      "(250000, 230)\n"
     ]
    }
   ],
   "source": [
    "tempData = features_train\n",
    "data = twoFeatureCombinationPower(tempData,1,features_train.shape[1])\n",
    "print(data.shape)\n",
    "data = FeaturePower(data,2,features_train.shape[1])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=labels_train[0:10000]\n",
    "tx=data[0:10000,:]\n",
    "initial_w=np.zeros((tx.shape[1], 1))\n",
    "max_iter=1000\n",
    "gamma=1\n",
    "lambda_=1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws, losses = learning_grad_descent(y, tx, initial_w, max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1) [ 0.69314718]\n",
      "Current iteration=0, loss=[[ 0.69314718]]\n",
      "(10000, 1) [-19520.89948282]\n",
      "Current iteration=1, loss=[[-18445.1383989]]\n",
      "(10000, 1) [-148400.21064397]\n",
      "Current iteration=2, loss=[[-110964.5823949]]\n",
      "(10000, 1) [-366997.85206887]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan\\Desktop\\Etudes\\GitHub\\ML_project1\\Notebook\\logistic_regression.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (np.exp(-x) + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=3, loss=[[-200712.22433335]]\n",
      "(10000, 1) [-440208.08463631]\n",
      "Current iteration=4, loss=[[-226272.38405902]]\n",
      "(10000, 1) [-463776.93889175]\n",
      "Current iteration=5, loss=[[-233751.76499419]]\n",
      "(10000, 1) [-470064.57698661]\n",
      "Current iteration=6, loss=[[-235168.48123273]]\n",
      "(10000, 1) [-470443.13023853]\n",
      "Current iteration=7, loss=[[-235222.81550474]]\n",
      "(10000, 1) [-470443.44226061]\n",
      "Current iteration=8, loss=[[-235222.81510544]]\n"
     ]
    }
   ],
   "source": [
    "w1,w2,loss=learning_by_penalized_gradient(y, tx, initial_w, max_iter, gamma, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w1/w2)\n",
    "print(w1.T.dot(w1),w2.T.dot(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(1000, 1) [-4107.84237244]\n",
    "Current iteration=46, loss=[[-2070.29659866]]\n",
    "(1000, 1) [-4115.127137]\n",
    "Current iteration=47, loss=[[-2070.32649888]]\n",
    "(1000, 1) [-4121.68579492]\n",
    "Current iteration=48, loss=[[-2070.33963515]]\n",
    "(1000, 1) [-4127.59049219]\n",
    "Current iteration=49, loss=[[-2070.34028172]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
