{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from proj1_helpers import * \n",
    "from play_with_data import * # Contains some functions that allow us to look the data and manipulate a little bit\n",
    "from pre_processing import * # Contains all the functions needed to do some pre-processing on the data\n",
    "from logistic_regression import * # Contains the functions for gradient descent, penalized gradient descent, stochastic gradient descent\n",
    "from cross_val import * # Contains the functions for the cross-validation\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_x, y, x = load_data(\"train.csv\")\n",
    "# ********* Pre-processing ******************\n",
    "x_whit = data_whitening(x) # Do the whitening \n",
    "x_whit[np.where(x_whit == -999.0)] = 0 # Remove the values of -999.0\n",
    "\n",
    "\n",
    "# ********* Adding combination of features or squared of feature ******\n",
    "x_temp = twoFeatureCombinationPower(x_whit, 1, x_whit.shape[1])\n",
    "x = FeaturePower(x_temp, 2, x_temp.shape[1])\n",
    "\n",
    "# **** End of the pre-processed by adding first column *********\n",
    "tx = build_model_data(x) # add the first column —> [1]\n",
    "\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.1\n",
    "'''In order to speed up the proces, we only take into account a ratio of the total samples    '''\n",
    "#CAUTION USE TRAINING DATA \n",
    "x_tr, x_te, y_tr, y_te = split_data(tx, y, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_w=np.zeros((tx.shape[1], 1))\n",
    "\n",
    "seed = 1\n",
    "k_fold = 10\n",
    "gammas = 0.8 #np.linspace(0.25,1,4) # Hyperparameter\n",
    "lambdas = np.logspace(-3,2,5)*5 # Hyperparameter\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "allValidationError = np.zeros([1, len(lambdas)])\n",
    "\n",
    "#for ind_g, g in enumerate(gammas):\n",
    "for ind_l, l in enumerate(lambdas):\n",
    "    validation_error = 0\n",
    "    print(\"gamma: {}\\tlambda: {}\".format(gammas,l))\n",
    "    for k in range(k_fold):\n",
    "        print(k)\n",
    "        validation_error = validation_error + cross_validation(y, tx, k_indices, k, gammas, l)\n",
    "    allValidationError[0, ind_l] = validation_error/k_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering with the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1,loss=learning_by_penalized_gradient(y, tx, initial_w, k, gamma, lambdas)id_test, features_test = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a58c4758f973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mid_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeatures_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_whitening\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Do the whitening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/iCloud Drive (archive)/Documents/EPFL/Master/Master 3/ML/Project 1 ML/ML_project1/Notebook/helpers.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(dataSetCSVfile)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# \"data\" will contain all the features for each samples so it will be a NxM matrix (with N = sumber of sample, M = number of features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         data = np.genfromtxt(\n\u001b[0;32m---> 27\u001b[0;31m             path_dataset, delimiter=\",\", skip_header=1, usecols=(np.arange(2,32)))\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mid_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         rows = list(\n\u001b[1;32m   1882\u001b[0m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[0;32m-> 1883\u001b[0;31m                   for (i, conv) in enumerate(converters)]))\n\u001b[0m\u001b[1;32m   1884\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m         rows = list(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         rows = list(\n\u001b[1;32m   1882\u001b[0m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[0;32m-> 1883\u001b[0;31m                   for (i, conv) in enumerate(converters)]))\n\u001b[0m\u001b[1;32m   1884\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m         rows = list(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1880\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m         rows = list(\n\u001b[0;32m-> 1882\u001b[0;31m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[0m\u001b[1;32m   1883\u001b[0m                   for (i, conv) in enumerate(converters)]))\n\u001b[1;32m   1884\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "id_test, features_test = load_data(\"test.csv\")\n",
    "\n",
    "features_test = data_whitening(x) # Do the whitening \n",
    "features_test[np.where(features_test == -999.0)] = 0 # Remove the values of -999.0\n",
    "\n",
    "\n",
    "# ********* Adding combination of features or squared of feature ******\n",
    "x_temp = twoFeatureCombinationPower(features_test, 1, features_test.shape[1])\n",
    "x = FeaturePower(x_temp, 2, x_temp.shape[1])\n",
    "\n",
    "# **** End of the pre-processed by adding first column *********\n",
    "tx = build_model_data(x) # add the first column —> [1]\n",
    "\n",
    "y_pred = predict_labels(w1, tx)\n",
    "y_pred[np.where(y_pred == 0)] = -1 # Here we transform the 0 label to -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(id_test, y_pred, 'submission_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
